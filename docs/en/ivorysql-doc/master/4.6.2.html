<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploy IvorySQL with IvorySQL Operator :: IvorySQL Document Site</title>
    <link rel="canonical" href="https://docs.ivorysql.org/ivorysql-doc/master/4.6.2.html">
    <link rel="prev" href="4.6.1.html">
    <link rel="next" href="4.6.4.html">
    <meta name="generator" content="Antora 3.1.7">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ivorysql.org">IvorySQL Document Site</a>
      <div class="navbar-item search hide-for-print">
        <div id="search-field" class="field">
          <input id="search-input" type="text" placeholder="Search the docs">
        </div>
      </div>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://www.ivorysql.org/">Home</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="ivorysql-doc" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="welcome.html">IvorySQL</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="welcome.html">Welcome</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="1.html">Release</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="2.html">About</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Getting Started with IvorySQL</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="3.1.html">Quick Start</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="3.2.html">Monitoring</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="3.3.html">Maintenance</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">IvorySQL Advanced Feature</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="4.1.html">Installation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="4.2.html">Cluster</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="4.5.html">Migration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="4.3.html">Developer</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Containerization</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="4.6.1.html">K8S deployment</a>
  </li>
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="4.6.2.html">Operator deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="4.6.4.html">Docker &amp; Podman deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="4.6.3.html">Docker Swarm &amp; Docker Compose deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="4.4.html">Operation Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Cloud Service Platform</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="4.7.1.html">IvorySQL Cloud Installation</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="4.7.2.html">IvorySQL Cloud Usage</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">IvorySQL Ecosystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="cpu_arch_adp.html">CPU Architecture Adaption</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="os_arch_adp.html">Operating System Adaption</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Eco Component Adaption</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.0.html">Overview</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.1.html">postgis</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.2.html">pgvector</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.3.html">pgddl(DDL Extractor)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.4.html">pg_cron</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.5.html">pgsql-http</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.6.html">plpgsql_check</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.7.html">pgroonga</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.8.html">pgaudit</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.9.html">pgrouting</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="5.10.html">system_stats</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">IvorySQL Architecture Design</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Query Processing</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.1.1.html">Dual Parser</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Compatibility Framework</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="7.1.html">Ivorysql frame design</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="7.2.html">GUC Framework</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="7.4.html">Dual-mode design</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.2.1.html">initdb Process</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Compatibility Features</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.1.html">like</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.3.html">RowID</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.2.html">OUT Parameter</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.4.html">%Type &amp; %Rowtype</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.5.html">NLS Parameters</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.6.html">Function and stored procedure</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.7.html">Nested Subfunctions</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.8.html">Force View</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.9.html">Case Conversion</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.10.html">sys_guid Function</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.11.html">Empty String to NULL</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.3.12.html">CALL INTO</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Built-in Functions</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.4.1.html">sys_context</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="6.4.2.html">userenv</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="6.5.html">GB18030 Character Set</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">List of Oracle compatible features</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.3.html">1、Case conversion</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.5.html">2、LIKE operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.6.html">3、anonymous block</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.7.html">4、functions and stored procedures</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.8.html">5、Built-in data types and built-in functions</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.9.html">6、ports and IP</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.10.html">7、XML Function</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.11.html">8、sequence</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.12.html">9、Package</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.13.html">10、Invisible Columns</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.14.html">11、RowID Column</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.15.html">12、OUT Parameter</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.16.html">13、%Type &amp; %Rowtype</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.17.html">14、NLS Parameters</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.18.html">15、Force View</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.19.html">16、Nested Subfunctions</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.20.html">17、sys_guid Function</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.21.html">18、Empty String to NULL</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="7.22.html">19、CALL INTO</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="8.html">Community contribution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="9.html">Tool Reference</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="10.html">FAQ</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">IvorySQL</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="welcome.html">IvorySQL</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="welcome.html">master</a>
        </li>
        <li class="version">
          <a href="../v4.6/v4.6/welcome.html">v4.6</a>
        </li>
        <li class="version">
          <a href="../v1.17/v1.17/welcome.html">v1.17</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="welcome.html">IvorySQL</a></li>
    <li>IvorySQL Advanced Feature</li>
    <li>Containerization</li>
    <li><a href="4.6.2.html">Operator deployment</a></li>
  </ul>
</nav>
  <div class="page-versions">
  <button class="version-menu-toggle" title="switch to Chinese">EN</button>
  <div class="version-menu">
    <a class="version is-current" href="">EN</a>
    <a class="version" href="../../../../cn/ivorysql-doc/master/4.6.2.html">CN</a>
  </div>
  </div>
  <div class="edit-this-page"><a href="https://github.com/IvorySQL/ivorysql_docs/edit/master/EN/modules/ROOT/pages/master/4.6.2.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Deploy IvorySQL with IvorySQL Operator</h1>
<div class="sect1">
<h2 id="operator-installation"><a class="anchor" href="#operator-installation"></a>1. Operator Installation</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Fork <a href="https://github.com/IvorySQL/ivory-operator">ivory-operator repository</a> and clone it to your host machine：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">YOUR_GITHUB_UN="&lt;your GitHub username&gt;"
git clone --depth 1 "git@github.com:${YOUR_GITHUB_UN}/ivory-operator.git"
cd ivory-operator</code></pre>
</div>
</div>
</li>
<li>
<p>Run the following commands：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl apply -k examples/kustomize/install/namespace
kubectl apply --server-side -k examples/kustomize/install/default</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="getting-started"><a class="anchor" href="#getting-started"></a>2. Getting Started</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Throughout this tutorial, we will be building on the example provided in the <code>examples/kustomize/ivory</code>.</p>
</div>
<div class="paragraph">
<p>When referring to a nested object within a YAML manifest, we will be using the <code>.</code> format similar to <code>kubectl explain</code>. For example, if we want to refer to the deepest element in this yaml file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  hippos:
    appetite: huge</code></pre>
</div>
</div>
<div class="paragraph">
<p>we would say <code>spec.hippos.appetite</code>.</p>
</div>
<div class="paragraph">
<p><code>kubectl explain</code> is your friend. You can use <code>kubectl explain ivorycluster</code> to introspect the <code>ivorycluster.ivory-operator.ivorysql.org</code> custom resource definition.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="create-an-ivory-cluster"><a class="anchor" href="#create-an-ivory-cluster"></a>3. Create an Ivory Cluster</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="create"><a class="anchor" href="#create"></a>3.1. Create</h3>
<div class="paragraph">
<p>Creating an Ivory cluster is pretty simple. Using the example in the <code>examples/kustomize/ivory</code> directory, all we have to do is run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>and IVYO will create a simple Ivory cluster named <code>hippo</code> in the <code>ivory-operator</code> namespace. You can track the status of your Ivory cluster using <code>kubectl describe</code> on the <code>ivoryclusters.ivory-operator.ivorysql.org</code> custom resource:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator describe ivoryclusters.ivory-operator.ivorysql.org hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>and you can track the state of the Ivory Pod using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get pods \
  --selector=ivory-operator.ivorysql.org/cluster=hippo,ivory-operator.ivorysql.org/instance</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="what-just-happened"><a class="anchor" href="#what-just-happened"></a>3.1.1. What Just Happened?</h4>
<div class="paragraph">
<p>IVYO created an Ivory cluster based on the information provided to it in the Kustomize manifests located in the <code>examples/kustomize/ivory</code> directory. Let&#8217;s better understand what happened by inspecting the <code>examples/kustomize/ivory/ivory.yaml</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>When we ran the <code>kubectl apply</code> command earlier, what we did was create a <code>ivorycluster</code> custom resource in Kubernetes. IVYO detected that we added a new <code>ivorycluster</code> resource and started to create all the objects needed to run Ivory in Kubernetes!</p>
</div>
<div class="paragraph">
<p>What else happened? IVYO read the value from <code>metadata.name</code> to provide the Ivory cluster with the name <code>hippo</code>. Additionally, IVYO knew which containers to use for Ivory and pgBackRest by looking at the values in <code>spec.image</code> and <code>spec.backups.pgbackrest.image</code> respectively. The value in <code>spec.postgresVersion</code> is important as it will help IVYO track which major version of Ivory you are using.</p>
</div>
<div class="paragraph">
<p>IVYO knows how many Ivory instances to create through the <code>spec.instances</code> section of the manifest. While <code>name</code> is optional, we opted to give it the name <code>instance1</code>. We could have also created multiple replicas and instances during cluster initialization, but we will cover that more when we discuss how to <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/high-availability.md">scale and create a HA Ivory cluster</a>.</p>
</div>
<div class="paragraph">
<p>A very important piece of your <code>ivorycluster</code> custom resource is the <code>dataVolumeClaimSpec</code> section. This describes the storage that your Ivory instance will use. It is modeled after the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volume Claim</a>. If you do not provide a <code>spec.instances.dataVolumeClaimSpec.storageClassName</code>, then the default storage class in your Kubernetes environment is used.</p>
</div>
<div class="paragraph">
<p>As part of creating an Ivory cluster, we also specify information about our backup archive. IVYO uses <a href="https://pgbackrest.org/">pgBackRest</a>, an open source backup and restore tool designed to handle terabyte-scale backups. As part of initializing our cluster, we can specify where we want our backups and archives (<a href="https://www.postgresql.org/docs/current/wal-intro.html">write-ahead logs or WAL</a>) stored. We will talk about this portion of the <code>ivorycluster</code> spec in greater depth in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/backups.md">disaster recovery</a> section of this tutorial, and also see how we can store backups in Amazon S3, Google GCS, and Azure Blob Storage.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting"><a class="anchor" href="#troubleshooting"></a>3.2. Troubleshooting</h3>
<div class="sect3">
<h4 id="ivorysql-pgbackrest-pods-stuck-in-pending-phase"><a class="anchor" href="#ivorysql-pgbackrest-pods-stuck-in-pending-phase"></a>3.2.1. IvorySQL / pgBackRest Pods Stuck in <code>Pending</code> Phase</h4>
<div class="paragraph">
<p>The most common occurrence of this is due to PVCs not being bound. Ensure that you have set up your storage options correctly in any <code>volumeClaimSpec</code>. You can always update your settings and reapply your changes with <code>kubectl apply</code>.</p>
</div>
<div class="paragraph">
<p>Also ensure that you have enough persistent volumes available: your Kubernetes administrator may need to provision more.</p>
</div>
<div class="paragraph">
<p>If you are on OpenShift, you may need to set <code>spec.openshift</code> to <code>true</code>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps"><a class="anchor" href="#next-steps"></a>3.3. Next Steps</h3>
<div class="paragraph">
<p>We&#8217;re up and running&#8201;&#8212;&#8201;now let&#8217;s <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/connect-cluster.md">connect to our Ivory cluster</a>!</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="connect-to-an-ivory-cluster"><a class="anchor" href="#connect-to-an-ivory-cluster"></a>4. Connect to an Ivory Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>It&#8217;s one thing to <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/create-cluster.md">create an Ivory cluster</a>; it&#8217;s another thing to connect to it. Let&#8217;s explore how IVYO makes it possible to connect to an Ivory cluster!</p>
</div>
<div class="sect2">
<h3 id="background-services-secrets-and-tls"><a class="anchor" href="#background-services-secrets-and-tls"></a>4.1. Background: Services, Secrets, and TLS</h3>
<div class="paragraph">
<p>IVYO creates a series of Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> to provide stable endpoints for connecting to your Ivory databases. These endpoints make it easy to provide a consistent way for your application to maintain connectivity to your data. To inspect what services are available, you can run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get svc --selector=ivory-operator.ivorysql.org/cluster=hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>will yield something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
hippo-ha          ClusterIP   10.103.73.92   &lt;none&gt;        5432/TCP   3h14m
hippo-ha-config   ClusterIP   None           &lt;none&gt;        &lt;none&gt;     3h14m
hippo-pods        ClusterIP   None           &lt;none&gt;        &lt;none&gt;     3h14m
hippo-primary     ClusterIP   None           &lt;none&gt;        5432/TCP   3h14m
hippo-replicas    ClusterIP   10.98.110.215  &lt;none&gt;        5432/TCP   3h14m</code></pre>
</div>
</div>
<div class="paragraph">
<p>You do not need to worry about most of these Services, as they are used to help manage the overall health of your Ivory cluster. For the purposes of connecting to your database, the Service of interest is called <code>hippo-primary</code>. Thanks to IVYO, you do not need to even worry about that, as that information is captured within a Secret!</p>
</div>
<div class="paragraph">
<p>When your Ivory cluster is initialized, IVYO will bootstrap a database and Ivory user that your application can access. This information is stored in a Secret named with the pattern <code>&lt;clusterName&gt;-pguser-&lt;userName&gt;</code>. For our <code>hippo</code> cluster, this Secret is called <code>hippo-pguser-hippo</code>. This Secret contains the information you need to connect your application to your Ivory database:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>user</code>: The name of the user account.</p>
</li>
<li>
<p><code>password</code>: The password for the user account.</p>
</li>
<li>
<p><code>dbname</code>: The name of the database that the user has access to by default.</p>
</li>
<li>
<p><code>host</code>: The name of the host of the database.
This references the <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> of the primary Ivory instance.</p>
</li>
<li>
<p><code>port</code>: The port that the database is listening on.</p>
</li>
<li>
<p><code>uri</code>: A <a href="https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING">PostgresSQL connection URI</a>
that provides all the information for logging into the Ivory database.</p>
</li>
<li>
<p><code>jdbc-uri</code>: A <a href="https://jdbc.postgresql.org/documentation/use/">PostgresSQL JDBC connection URI</a> that provides
all the information for logging into the Ivory database via the JDBC driver.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All connections are over TLS. IVYO provides its own certificate authority (CA) to allow you to securely connect your applications to your Ivory clusters. This allows you to use the <a href="https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS"><code>verify-full</code> "SSL mode"</a> of Ivory, which provides eavesdropping protection and prevents MITM attacks. You can also choose to bring your own CA, which is described later in this tutorial in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md">Customize Cluster</a> section.</p>
</div>
<div class="sect3">
<h4 id="modifying-service-type-nodeport-value-and-metadata"><a class="anchor" href="#modifying-service-type-nodeport-value-and-metadata"></a>4.1.1. Modifying Service Type, NodePort Value and Metadata</h4>
<div class="paragraph">
<p>By default, IVYO deploys Services with the <code>ClusterIP</code> Service type. Based on how you want to expose your database,
you may want to modify the Services to use a different
<a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service type</a>
and <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort value</a>.</p>
</div>
<div class="paragraph">
<p>You can modify the Services that IVYO manages from the following attributes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.service</code> - this manages the Service for connecting to an Ivory primary.</p>
</li>
<li>
<p><code>spec.userInterface.pgAdmin.service</code> - this manages the Service for connecting to the pgAdmin management tool.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, say you want to set the Ivory primary to use a <code>NodePort</code> service, a specific <code>nodePort</code> value, and set
a specific annotation and label, you would add the following to your manifest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  service:
    metadata:
      annotations:
        my-annotation: value1
      labels:
        my-label: value2
    type: NodePort
    nodePort: 32000</code></pre>
</div>
</div>
<div class="paragraph">
<p>For our <code>hippo</code> cluster, you would see the Service type and nodePort modification as well as the annotation and label.
For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get svc --selector=ivory-operator.ivorysql.org/cluster=hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>will yield something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hippo-ha          NodePort    10.105.57.191   &lt;none&gt;        5432:32000/TCP   48s
hippo-ha-config   ClusterIP   None            &lt;none&gt;        &lt;none&gt;           48s
hippo-pods        ClusterIP   None            &lt;none&gt;        &lt;none&gt;           48s
hippo-primary     ClusterIP   None            &lt;none&gt;        5432/TCP         48s
hippo-replicas    ClusterIP   10.106.18.99    &lt;none&gt;        5432/TCP         48s</code></pre>
</div>
</div>
<div class="paragraph">
<p>and the top of the output from running</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator describe svc hippo-ha</code></pre>
</div>
</div>
<div class="paragraph">
<p>will show our custom annotation and label have been added:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">Name:              hippo-ha
Namespace:         ivory-operator
Labels:            my-label=value2
                   ivory-operator.ivorysql.org/cluster=hippo
                   ivory-operator.ivorysql.org/patroni=hippo-ha
Annotations:       my-annotation: value1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that setting the <code>nodePort</code> value is not allowed when using the (default) <code>ClusterIP</code> type, and it must be in-range
and not otherwise in use or the operation will fail. Additionally, be aware that any annotations or labels provided here
will win in case of conflicts with any annotations or labels a user configures elsewhere.</p>
</div>
<div class="paragraph">
<p>Finally, if you are exposing your Services externally and are relying on TLS
verification, you will need to use the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md#customize-tls">custom TLS</a>
features of IVYO).</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="connect-an-application"><a class="anchor" href="#connect-an-application"></a>4.2. Connect an Application</h3>
<div class="paragraph">
<p>For this tutorial, we are going to connect <a href="https://www.keycloak.org/">Keycloak</a>, an open source
identity management application. Keycloak can be deployed on Kubernetes and is backed by an Ivory
database. We provide an example of deploying Keycloak andan ivorycluster, the manifest below deploys it using our <code>hippo</code> cluster that is already running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply --filename=- &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keycloak
  namespace: ivory-operator
  labels:
    app.kubernetes.io/name: keycloak
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: keycloak
  template:
    metadata:
      labels:
        app.kubernetes.io/name: keycloak
    spec:
      containers:
      - image: quay.io/keycloak/keycloak:latest
        args: ["start-dev"]
        name: keycloak
        env:
        - name: DB_VENDOR
          value: "ivory"
        - name: DB_ADDR
          valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: host } }
        - name: DB_PORT
          valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: port } }
        - name: DB_DATABASE
          valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: dbname } }
        - name: DB_USER
          valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: user } }
        - name: DB_PASSWORD
          valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: password } }
        - name: KEYCLOAK_ADMIN
          value: "admin"
        - name: KEYCLOAK_ADMIN_PASSWORD
          value: "admin"
        - name: KC_PROXY
          value: "edge"
        ports:
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        readinessProbe:
          httpGet:
            path: /realms/master
            port: 8080
      restartPolicy: Always
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice this part of the manifest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: DB_ADDR
  valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: host } }
- name: DB_PORT
  valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: port } }
- name: DB_DATABASE
  valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: dbname } }
- name: DB_USER
  valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: user } }
- name: DB_PASSWORD
  valueFrom: { secretKeyRef: { name: hippo-pguser-hippo, key: password } }</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above manifest shows how all of these values are derived from the <code>hippo-pguser-hippo</code> Secret. This means that we do not need to know any of the connection credentials or have to insecurely pass them around&#8201;&#8212;&#8201;they are made directly available to the application!</p>
</div>
<div class="paragraph">
<p>Using this method, you can tie application directly into your GitOps pipeline that connect to Ivory without any prior knowledge of how IVYO will deploy Ivory: all of the information your application needs is propagated into the Secret!</p>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-2"><a class="anchor" href="#next-steps-2"></a>4.3. Next Steps</h3>
<div class="paragraph">
<p>Now that we have seen how to connect an application to a cluster, let&#8217;s learn how to create a <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/high-availability.md">high availability Ivory</a> cluster!</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="high-availability"><a class="anchor" href="#high-availability"></a>5. High Availability</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ivory is known for its reliability: it is very stable and typically "just works." However, there are many things that can happen in a distributed environment like Kubernetes that can affect Ivory uptime, including:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The database storage disk fails or some other hardware failure occurs</p>
</li>
<li>
<p>The network on which the database resides becomes unreachable</p>
</li>
<li>
<p>The host operating system becomes unstable and crashes</p>
</li>
<li>
<p>A key database file becomes corrupted</p>
</li>
<li>
<p>A data center is lost</p>
</li>
<li>
<p>A Kubernetes component (e.g. a Service) is accidentally deleted</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There may also be downtime events that are due to the normal case of operations, such as performing a minor upgrade, security patching of operating system, hardware upgrade, or other maintenance.</p>
</div>
<div class="paragraph">
<p>The good news: IVYO is prepared for this, and your Ivory cluster is protected from many of these scenarios. However, to maximize your high availability (HA), let&#8217;s first scale up your Ivory cluster.</p>
</div>
<div class="sect2">
<h3 id="ha-ivory-adding-replicas-to-your-ivory-cluster"><a class="anchor" href="#ha-ivory-adding-replicas-to-your-ivory-cluster"></a>5.1. HA Ivory: Adding Replicas to your Ivory Cluster</h3>
<div class="paragraph">
<p>IVYO provides several ways to add replicas to make a HA cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Increase the <code>spec.instances.replicas</code> value</p>
</li>
<li>
<p>Add an additional entry in <code>spec.instances</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the purposes of this tutorial, we will go with the first method and set <code>spec.instances.replicas</code> to <code>2</code>. Your manifest should look similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply these updates to your Ivory cluster with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>Within moment, you should see a new Ivory instance initializing! You can see all of your Ivory Pods for the <code>hippo</code> cluster by running the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get pods \
  --selector=ivory-operator.ivorysql.org/cluster=hippo,ivory-operator.ivorysql.org/instance-set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s test our high availability set up.</p>
</div>
</div>
<div class="sect2">
<h3 id="testing-your-ha-cluster"><a class="anchor" href="#testing-your-ha-cluster"></a>5.2. Testing Your HA Cluster</h3>
<div class="paragraph">
<p>An important part of building a resilient Ivory environment is testing its resiliency, so let&#8217;s run a few tests to see how IVYO performs under pressure!</p>
</div>
<div class="sect3">
<h4 id="test-1-remove-a-service"><a class="anchor" href="#test-1-remove-a-service"></a>5.2.1. Test #1: Remove a Service</h4>
<div class="paragraph">
<p>Let&#8217;s try removing the primary Service that our application is connected to. This test does not actually require a HA Ivory cluster, but it will demonstrate IVYO&#8217;s ability to react to environmental changes and heal things to ensure your applications can stay up.</p>
</div>
<div class="paragraph">
<p>Recall in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/connect-cluster.md">connecting a Ivory cluster</a> that we observed the Services that IVYO creates, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get svc \
  --selector=ivory-operator.ivorysql.org/cluster=hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>yields something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
hippo-ha          ClusterIP   10.103.73.92   &lt;none&gt;        5432/TCP   4h8m
hippo-ha-config   ClusterIP   None           &lt;none&gt;        &lt;none&gt;     4h8m
hippo-pods        ClusterIP   None           &lt;none&gt;        &lt;none&gt;     4h8m
hippo-primary     ClusterIP   None           &lt;none&gt;        5432/TCP   4h8m
hippo-replicas    ClusterIP   10.98.110.215  &lt;none&gt;        5432/TCP   4h8m</code></pre>
</div>
</div>
<div class="paragraph">
<p>We also mentioned that the application is connected to the <code>hippo-primary</code> Service. What happens if we were to delete this Service?</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator delete svc hippo-primary</code></pre>
</div>
</div>
<div class="paragraph">
<p>This would seem like it could create a downtime scenario, but run the above selector again:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get svc \
  --selector=ivory-operator.ivorysql.org/cluster=hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
hippo-ha          ClusterIP   10.103.73.92   &lt;none&gt;        5432/TCP   4h8m
hippo-ha-config   ClusterIP   None           &lt;none&gt;        &lt;none&gt;     4h8m
hippo-pods        ClusterIP   None           &lt;none&gt;        &lt;none&gt;     4h8m
hippo-primary     ClusterIP   None           &lt;none&gt;        5432/TCP   3s
hippo-replicas    ClusterIP   10.98.110.215  &lt;none&gt;        5432/TCP   4h8m</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wow&#8201;&#8212;&#8201;IVYO detected that the primary Service was deleted and it recreated it! Based on how your application connects to Ivory, it may not have even noticed that this event took place!</p>
</div>
<div class="paragraph">
<p>Now let&#8217;s try a more extreme downtime event.</p>
</div>
</div>
<div class="sect3">
<h4 id="test-2-remove-the-primary-statefulset"><a class="anchor" href="#test-2-remove-the-primary-statefulset"></a>5.2.2. Test #2: Remove the Primary StatefulSet</h4>
<div class="paragraph">
<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> are a Kubernetes object that provide helpful mechanisms for managing Pods that interface with stateful applications, such as databases. They provide a stable mechanism for managing Pods to help ensure data is retrievable in a predictable way.</p>
</div>
<div class="paragraph">
<p>What happens if we remove the StatefulSet that is pointed to the Pod that represents the Ivory primary? First, let&#8217;s determine which Pod is the primary. We&#8217;ll store it in an environmental variable for convenience.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">PRIMARY_POD=$(kubectl -n ivory-operator get pods \
  --selector=ivory-operator.ivorysql.org/role=master \
  -o jsonpath='{.items[*].metadata.labels.ivory-operator\.ivorysql\.org/instance}')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Inspect the environmental variable to see which Pod is the current primary:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">echo $PRIMARY_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>should yield something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">hippo-instance1-zj5s</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can use the value above to delete the StatefulSet associated with the current Ivory primary instance:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl delete sts -n ivory-operator "${PRIMARY_POD}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s see what happens. Try getting all of the StatefulSets for the Ivory instances in the <code>hippo</code> cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get sts -n ivory-operator \
  --selector=ivory-operator.ivorysql.org/cluster=hippo,ivory-operator.ivorysql.org/instance</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">NAME                   READY   AGE
hippo-instance1-6kbw   1/1     15m
hippo-instance1-zj5s   0/1     1s</code></pre>
</div>
</div>
<div class="paragraph">
<p>IVYO recreated the StatefulSet that was deleted! After this "catastrophic" event, IVYO proceeds to heal the Ivory instance so it can rejoin the cluster. We cover the high availability process in greater depth later in the documentation.</p>
</div>
<div class="paragraph">
<p>What about the other instance? We can see that it became the new primary though the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator get pods \
  --selector=ivory-operator.ivorysql.org/role=master \
  -o jsonpath='{.items[*].metadata.labels.ivory-operator\.ivorysql\.org/instance}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>which should yield something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">hippo-instance1-6kbw</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can test that the failover successfully occurred in a few ways. You can connect to the example Keycloak application that we <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/connect-cluster.md">deployed in the previous section</a>. Based on Keycloak&#8217;s connection retry logic, you may need to wait a moment for it to reconnect, but you will see it connected and resume being able to read and write data. You can also connect to the Ivory instance directly and execute the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">SELECT NOT pg_catalog.pg_is_in_recovery() is_primary;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If it returns <code>true</code> (or <code>t</code>), then the Ivory instance is a primary!</p>
</div>
<div class="paragraph">
<p>What if IVYO was down during the downtime event? Failover would still occur: the Ivory HA system works independently of IVYO and can maintain its own uptime. IVYO will still need to assist with some of the healing aspects, but your application will still maintain read/write connectivity to your Ivory cluster!</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="synchronous-replication"><a class="anchor" href="#synchronous-replication"></a>5.3. Synchronous Replication</h3>
<div class="paragraph">
<p>IvorySQL supports synchronous replication, which is a replication mode designed to limit the risk of transaction loss. Synchronous replication waits for a transaction to be written to at least one additional server before it considers the transaction to be committed. For more information on synchronous replication, please read about IVYO&#8217;s <a href="https://github.com/CrunchyData/postgres-operator/blob/master/docs/content/architecture/high-availability.md#synchronous-replication-guarding-against-transactions-loss">high availability architecture</a></p>
</div>
<div class="paragraph">
<p>To add synchronous replication to your Ivory cluster, you can add the following to your spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    dynamicConfiguration:
      synchronous_mode: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>While PostgreSQL defaults <a href="https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT"><code>synchronous_commit</code></a> to <code>on</code>, you may also want to explicitly set it, in which case the above block becomes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    dynamicConfiguration:
      synchronous_mode: true
      postgresql:
        parameters:
          synchronous_commit: "on"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that Patroni, which manages many aspects of the cluster&#8217;s availability, will favor availability over synchronicity. This means that if a synchronous replica goes down, Patroni will allow for asynchronous replication to continue as well as writes to the primary. However, if you want to disable all writing if there are no synchronous replicas available, you would have to enable <code>synchronous_mode_strict</code>, i.e.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    dynamicConfiguration:
      synchronous_mode: true
      synchronous_mode_strict: true</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="affinity"><a class="anchor" href="#affinity"></a>5.4. Affinity</h3>
<div class="paragraph">
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Kubernetes affinity</a> rules, which include Pod anti-affinity and Node affinity, can help you to define where you want your workloads to reside. Pod anti-affinity is important for high availability: when used correctly, it ensures that your Ivory instances are distributed amongst different Nodes. Node affinity can be used to assign instances to specific Nodes, e.g. to utilize hardware that&#8217;s optimized for databases.</p>
</div>
<div class="sect3">
<h4 id="understanding-pod-labels"><a class="anchor" href="#understanding-pod-labels"></a>5.4.1. Understanding Pod Labels</h4>
<div class="paragraph">
<p>IVYO sets up several labels for Ivory cluster management that can be used for Pod anti-affinity or affinity rules in general. These include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ivory-operator.ivorysql.org/cluster</code>: This is assigned to all managed Pods in a Ivory cluster. The value of this label is the name of your Ivory cluster, in this case: <code>hippo</code>.</p>
</li>
<li>
<p><code>ivory-operator.ivorysql.org/instance-set</code>: This is assigned to all Ivory instances within a group of <code>spec.instances</code>. In the example above, the value of this label is <code>instance1</code>. If you do not assign a label, the value is automatically set by IVYO using a <code>NN</code> format, e.g. <code>00</code>.</p>
</li>
<li>
<p><code>ivory-operator.ivorysql.org/instance</code>: This is a unique label assigned to each Ivory instance containing the name of the Ivory instance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s look at how we can set up affinity rules for our Ivory cluster to help improve high availability.</p>
</div>
</div>
<div class="sect3">
<h4 id="pod-anti-affinity"><a class="anchor" href="#pod-anti-affinity"></a>5.4.2. Pod Anti-affinity</h4>
<div class="paragraph">
<p>Kubernetes has two types of Pod anti-affinity:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Preferred: With preferred (<code>preferredDuringSchedulingIgnoredDuringExecution</code>) Pod anti-affinity, Kubernetes will make a best effort to schedule Pods matching the anti-affinity rules to different Nodes. However, if it is not possible to do so, then Kubernetes may schedule one or more Pods to the same Node.</p>
</li>
<li>
<p>Required: With required (<code>requiredDuringSchedulingIgnoredDuringExecution</code>) Pod anti-affinity, Kubernetes mandates that each Pod matching the anti-affinity rules <strong>must</strong> be scheduled to different Nodes. However, a Pod may not be scheduled if Kubernetes cannot find a Node that does not contain a Pod matching the rules.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There is a trade-off with these two types of pod anti-affinity: while "required" anti-affinity will ensure that all the matching Pods are scheduled on different Nodes, if Kubernetes cannot find an available Node, your Ivory instance may not be scheduled. Likewise, while "preferred" anti-affinity will make a best effort to scheduled your Pods on different Nodes, Kubernetes may compromise and schedule more than one Ivory instance of the same cluster on the same Node.</p>
</div>
<div class="paragraph">
<p>By understanding these trade-offs, the makeup of your Kubernetes cluster, and your requirements, you can choose the method that makes the most sense for your Ivory deployment. We&#8217;ll show examples of both methods below!</p>
</div>
<div class="sect4">
<h5 id="using-preferred-pod-anti-affinity"><a class="anchor" href="#using-preferred-pod-anti-affinity"></a>5.4.2.1. Using Preferred Pod Anti-Affinity</h5>
<div class="paragraph">
<p>First, let&#8217;s deploy our Ivory cluster with preferred Pod anti-affinity. Note that if you have a single-node Kubernetes cluster, you will not see your Ivory instances deployed to different nodes. However, your Ivory instances <em>will</em> be deployed.</p>
</div>
<div class="paragraph">
<p>We can set up our HA Ivory cluster with preferred Pod anti-affinity like so:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  ivory-operator.ivorysql.org/cluster: hippo
                  ivory-operator.ivorysql.org/instance-set: instance1
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply those changes in your Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s take a closer look at this section:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      podAffinityTerm:
        topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            ivory-operator.ivorysql.org/cluster: hippo
            ivory-operator.ivorysql.org/instance-set: instance1</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>spec.instances.affinity.podAntiAffinity</code> follows the standard Kubernetes <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Pod anti-affinity spec</a>. The values for the <code>matchLabels</code> are derived from what we described in the previous section: <code>ivory-operator.ivorysql.org/cluster</code> is set to our cluster name of <code>hippo</code>, and <code>ivory-operator.ivorysql.org/instance-set</code> is set to the instance set name of <code>instance1</code>. We choose a <code>topologyKey</code> of <code>kubernetes.io/hostname</code>, which is standard in Kubernetes clusters.</p>
</div>
<div class="paragraph">
<p>Preferred Pod anti-affinity will perform a best effort to schedule your Ivory Pods to different nodes. Let&#8217;s see how you can require your Ivory Pods to be scheduled to different nodes.</p>
</div>
</div>
<div class="sect4">
<h5 id="using-required-pod-anti-affinity"><a class="anchor" href="#using-required-pod-anti-affinity"></a>5.4.2.2. Using Required Pod Anti-Affinity</h5>
<div class="paragraph">
<p>Required Pod anti-affinity forces Kubernetes to scheduled your Ivory Pods to different Nodes. Note that if Kubernetes is unable to schedule all Pods to different Nodes, some of your Ivory instances may become unavailable.</p>
</div>
<div class="paragraph">
<p>Using the previous example, let&#8217;s indicate to Kubernetes that we want to use required Pod anti-affinity for our Ivory clusters:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                ivory-operator.ivorysql.org/cluster: hippo
                ivory-operator.ivorysql.org/instance-set: instance1
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply those changes in your Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>If you are in a single Node Kubernetes clusters, you will notice that not all of your Ivory instance Pods will be scheduled. This is due to the <code>requiredDuringSchedulingIgnoredDuringExecution</code> preference. However, if you have enough Nodes available, you will see the Ivory instance Pods scheduled to different Nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get pods -n ivory-operator -o wide \
  --selector=ivory-operator.ivorysql.org/cluster=hippo,ivory-operator.ivorysql.org/instance</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="node-affinity"><a class="anchor" href="#node-affinity"></a>5.4.3. Node Affinity</h4>
<div class="paragraph">
<p>Node affinity can be used to assign your Ivory instances to Nodes with specific hardware or to guarantee a Ivory instance resides in a specific zone. Node affinity can be set within the <code>spec.instances.affinity.nodeAffinity</code> attribute, following the standard Kubernetes <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">node affinity spec</a>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s see an example with required Node affinity. Let&#8217;s say we have a set of Nodes that are reserved for database usage that have a label <code>workload-role=db</code>. We can create a Ivory cluster with a required Node affinity rule to scheduled all of the databases to those Nodes using the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: workload-role
                operator: In
                values:
                - db
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="pod-topology-spread-constraints"><a class="anchor" href="#pod-topology-spread-constraints"></a>5.5. Pod Topology Spread Constraints</h3>
<div class="paragraph">
<p>In addition to affinity and anti-affinity settings, <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Kubernetes Pod Topology Spread Constraints</a> can also help you to define where you want your workloads to reside. However, while PodAffinity allows any number of Pods to be added to a qualifying topology domain, and PodAntiAffinity allows only one Pod to be scheduled into a single topology domain, topology spread constraints allow you to distribute Pods across different topology domains with a finer level of control.</p>
</div>
<div class="sect3">
<h4 id="api-field-configuration"><a class="anchor" href="#api-field-configuration"></a>5.5.1. API Field Configuration</h4>
<div class="paragraph">
<p>The spread constraint <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods">API fields</a> can be configured for instance, PgBouncer and pgBackRest repo host pods. The basic configuration is as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">      topologySpreadConstraints:
      - maxSkew: &lt;integer&gt;
        topologyKey: &lt;string&gt;
        whenUnsatisfiable: &lt;string&gt;
        labelSelector: &lt;object&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>where "maxSkew" describes the maximum degree to which Pods can be unevenly distributed, "topologyKey" is the key that defines a topology in the Nodes' Labels, "whenUnsatisfiable" specifies what action should be taken when "maxSkew" can&#8217;t be satisfied, and "labelSelector" is used to find matching Pods.</p>
</div>
</div>
<div class="sect3">
<h4 id="example-spread-constraints"><a class="anchor" href="#example-spread-constraints"></a>5.5.2. Example Spread Constraints</h4>
<div class="paragraph">
<p>To help illustrate how you might use this with your cluster, we can review examples for configuring spread constraints on our Instance and pgBackRest repo host Pods. For this example, assume we have a three node Kubernetes cluster where the first node is labeled with <code>my-node-label=one</code>, the second node is labeled with <code>my-node-label=two</code> and the final node is labeled <code>my-node-label=three</code>. The label key <code>my-node-label</code> will function as our <code>topologyKey</code>. Note all three nodes in our examples will be schedulable, so a Pod could live on any of the three Nodes.</p>
</div>
<div class="sect4">
<h5 id="instance-pod-spread-constraints"><a class="anchor" href="#instance-pod-spread-constraints"></a>5.5.2.1. Instance Pod Spread Constraints</h5>
<div class="paragraph">
<p>To begin, we can set our topology spread constraints on our cluster Instance Pods. Given this configuration</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  instances:
    - name: instance1
      replicas: 5
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: my-node-label
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              ivory-operator.ivorysql.org/instance-set: instance1</code></pre>
</div>
</div>
<div class="paragraph">
<p>we will expect 5 Instance pods to be created. Each of these Pods will have the standard <code>ivory-operator.ivorysql.org/instance-set: instance1</code> Label set, so each Pod will be properly counted when determining the <code>maxSkew</code>. Since we have 3 nodes with a <code>maxSkew</code> of 1 and we&#8217;ve set <code>whenUnsatisfiable</code> to <code>DoNotSchedule</code>, we should see 2 Pods on 2 of the nodes and 1 Pod on the remaining Node, thus ensuring our Pods are distributed as evenly as possible.</p>
</div>
</div>
<div class="sect4">
<h5 id="pgbackrest-repo-pod-spread-constraints"><a class="anchor" href="#pgbackrest-repo-pod-spread-constraints"></a>5.5.2.2. pgBackRest Repo Pod Spread Constraints</h5>
<div class="paragraph">
<p>We can also set topology spread constraints on our cluster&#8217;s pgBackRest repo host pod. While we normally will only have a single pod per cluster, we could use a more generic label to add a preference that repo host Pods from different clusters are distributed among our Nodes. For example, by setting our <code>matchLabel</code> value to <code>ivory-operator.ivorysql.org/pgbackrest: ""</code> and our <code>whenUnsatisfiable</code> value to <code>ScheduleAnyway</code>, we will allow our repo host Pods to be scheduled no matter what Nodes may be available, but attempt to minimize skew as much as possible.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">      repoHost:
        topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: my-node-label
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              ivory-operator.ivorysql.org/pgbackrest: ""</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="putting-it-all-together"><a class="anchor" href="#putting-it-all-together"></a>5.5.2.3. Putting it All Together</h5>
<div class="paragraph">
<p>Now that each of our Pods has our desired Topology Spread Constraints defined, let&#8217;s put together a complete cluster definition:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 5
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: my-node-label
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              ivory-operator.ivorysql.org/instance-set: instance1
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1G
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repoHost:
        topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: my-node-label
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              ivory-operator.ivorysql.org/pgbackrest: ""
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1G</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can then apply those changes in your Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>Once your cluster finishes deploying, you can check that your Pods are assigned to the correct Nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get pods -n ivory-operator -o wide --selector=ivory-operator.ivorysql.org/cluster=hippo</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-3"><a class="anchor" href="#next-steps-3"></a>5.6. Next Steps</h3>
<div class="paragraph">
<p>We&#8217;ve now seen how IVYO helps your application stay "always on" with your Ivory database. Now let&#8217;s explore how IVYO can minimize or eliminate downtime for operations that would normally cause that, such as <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/resize-cluster.md">resizing your Ivory cluster</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="resize-an-ivory-cluster"><a class="anchor" href="#resize-an-ivory-cluster"></a>6. Resize an Ivory Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You did it&#8201;&#8212;&#8201;the application is a success! Traffic is booming, so much so that you need to add more resources to your Ivory cluster. However, you&#8217;re worried that any resize operation may cause downtime and create a poor experience for your end users.</p>
</div>
<div class="paragraph">
<p>This is where IVYO comes in: IVYO will help orchestrate rolling out any potentially disruptive changes to your cluster to minimize or eliminate downtime for your application. To do so, we will assume that you have <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/high-availability.md">deployed a high availability Ivory cluster</a> as described in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/high-availability.md">previous section</a>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s dive in.</p>
</div>
<div class="sect2">
<h3 id="resize-memory-and-cpu"><a class="anchor" href="#resize-memory-and-cpu"></a>6.1. Resize Memory and CPU</h3>
<div class="paragraph">
<p>Memory and CPU resources are an important component for vertically scaling your Ivory cluster.
Coupled with <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md">tweaks to your Ivory configuration file</a>,
allocating more memory and CPU to your cluster can help it to perform better under load.</p>
</div>
<div class="paragraph">
<p>It&#8217;s important for instances in the same high availability set to have the same resources.
IVYO lets you adjust CPU and memory within the <code>resources</code> sections of the <code>ivoryclusters.ivory-operator.ivorysql.org</code> custom resource. These include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.instances.resources</code> section, which sets the resource values for the IvorySQL container,
as well as any init containers in the associated pod and containers created by the <code>pgDataVolume</code> and <code>pgWALVolume</code> data migration jobs.</p>
</li>
<li>
<p><code>spec.instances.sidecars.replicaCertCopy.resources</code> section, which sets the resources for the <code>replica-cert-copy</code> sidecar container.</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.repoHost.resources</code> section, which sets the resources for the pgBackRest repo host container,
as well as any init containers in the associated pod and containers created by the <code>pgBackRestVolume</code> data migration job.</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.sidecars.pgbackrest.resources</code> section, which sets the resources for the <code>pgbackrest</code> sidecar container.</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.sidecars.pgbackrestConfig.resources</code> section, which sets the resources for the <code>pgbackrest-config</code> sidecar container.</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.jobs.resources</code> section, which sets the resources for any pgBackRest backup job.</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.restore.resources</code> section, which sets the resources for manual pgBackRest restore jobs.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.resources</code> section, which sets the resources for pgBackRest restore jobs created during the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/clone-cluster.md">cloning</a> process.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The layout of these <code>resources</code> sections should be familiar: they follow the same pattern as the standard Kubernetes structure for setting <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">container resources</a>. Note that these settings also allow for the configuration of <a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">QoS classes</a>.</p>
</div>
<div class="paragraph">
<p>For example, using the <code>spec.instances.resources</code> section, let&#8217;s say we want to update our <code>hippo</code> Ivory cluster so that each instance has a limit of <code>2.0</code> CPUs and <code>4Gi</code> of memory. We can make the following changes to the manifest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>In particular, we added the following to <code>spec.instances</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">resources:
  limits:
    cpu: 2.0
    memory: 4Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply these updates to your Ivory cluster with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, let&#8217;s watch how the rollout happens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch "kubectl -n ivory-operator get pods \
  --selector=ivory-operator.ivorysql.org/cluster=hippo,ivory-operator.ivorysql.org/instance \
  -o=jsonpath='{range .items[*]}{.metadata.name}{\"\t\"}{.metadata.labels.ivory-operator\.ivorysql\.org/role}{\"\t\"}{.status.phase}{\"\t\"}{.spec.containers[].resources.limits}{\"\n\"}{end}'"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Observe how each Pod is terminated one-at-a-time. This is part of a "rolling update". Because updating the resources of a Pod is a destructive action, IVYO first applies the CPU and memory changes to the replicas. IVYO ensures that the changes are successfully applied to a replica instance before moving on to the next replica.</p>
</div>
<div class="paragraph">
<p>Once all of the changes are applied, IVYO will perform a "controlled switchover": it will promote a replica to become a primary, and apply the changes to the final Ivory instance.</p>
</div>
<div class="paragraph">
<p>By rolling out the changes in this way, IVYO ensures there is minimal to zero disruption to your application: you are able to successfully roll out updates and your users may not even notice!</p>
</div>
</div>
<div class="sect2">
<h3 id="resize-pvc"><a class="anchor" href="#resize-pvc"></a>6.2. Resize PVC</h3>
<div class="paragraph">
<p>Your application is a success! Your data continues to grow, and it&#8217;s becoming apparently that you need more disk. That&#8217;s great: you can resize your PVC directly on your <code>ivoryclusters.ivory-operator.ivorysql.org</code> custom resource with minimal to zero downtime.</p>
</div>
<div class="paragraph">
<p>PVC resizing, also known as <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">volume expansion</a>, is a function of your storage class: it must support volume resizing. Additionally, PVCs can only be <strong>sized up</strong>: you cannot shrink the size of a PVC.</p>
</div>
<div class="paragraph">
<p>You can adjust PVC sizes on all of the managed storage instances in a Ivory instance that are using Kubernetes storage. These include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.instances.dataVolumeClaimSpec.resources.requests.storage</code>: The Ivory data directory (aka your database).</p>
</li>
<li>
<p><code>spec.backups.pgbackrest.repos.volume.volumeClaimSpec.resources.requests.storage</code>: The pgBackRest repository when using "volume" storage</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The above should be familiar: it follows the same pattern as the standard <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Kubernetes PVC</a> structure.</p>
</div>
<div class="paragraph">
<p>For example, let&#8217;s say we want to update our <code>hippo</code> Ivory cluster so that each instance now uses a <code>10Gi</code> PVC and our backup repository uses a <code>20Gi</code> PVC. We can do so with the following markup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 20Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>In particular, we added the following to <code>spec.instances</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">dataVolumeClaimSpec:
  resources:
    requests:
      storage: 10Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>and added the following to <code>spec.backups.pgbackrest.repos.volume</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">volumeClaimSpec:
  accessModes:
  - "ReadWriteOnce"
  resources:
    requests:
      storage: 20Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply these updates to your Ivory cluster with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="resize-pvcs-with-storageclass-that-does-not-allow-expansion"><a class="anchor" href="#resize-pvcs-with-storageclass-that-does-not-allow-expansion"></a>6.2.1. Resize PVCs With StorageClass That Does Not Allow Expansion</h4>
<div class="paragraph">
<p>Not all Kubernetes Storage Classes allow for <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">volume expansion</a>. However, with IVYO, you can still resize your Ivory cluster data volumes even if your storage class does not allow it!</p>
</div>
<div class="paragraph">
<p>Let&#8217;s go back to the previous example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 20Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>First, create a new instance that has the larger volume size. Call this instance <code>instance2</code>. The manifest would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
    - name: instance2
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 20Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Take note of the block that contains <code>instance2</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: instance2
  replicas: 2
  resources:
    limits:
      cpu: 2.0
      memory: 4Gi
  dataVolumeClaimSpec:
    accessModes:
    - "ReadWriteOnce"
    resources:
      requests:
        storage: 10Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>This creates a second set of two Ivory instances, both of which come up as replicas, that have a larger PVC.</p>
</div>
<div class="paragraph">
<p>Once this new instance set is available and they are caught to the primary, you can then apply the following manifest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance2
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 20Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will promote one of the instances with the larger PVC to be the new primary and remove the instances with the smaller PVCs!</p>
</div>
<div class="paragraph">
<p>This method can also be used to shrink PVCs to use a smaller amount.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting-2"><a class="anchor" href="#troubleshooting-2"></a>6.3. Troubleshooting</h3>
<div class="sect3">
<h4 id="ivory-pod-cant-be-scheduled"><a class="anchor" href="#ivory-pod-cant-be-scheduled"></a>6.3.1. Ivory Pod Can&#8217;t Be Scheduled</h4>
<div class="paragraph">
<p>There are many reasons why a IvorySQL Pod may not be scheduled:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Resources are unavailable</strong>. Ensure that you have a Kubernetes <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">Node</a> with enough resources to satisfy your memory or CPU Request.</p>
</li>
<li>
<p><strong>PVC cannot be provisioned</strong>. Ensure that you request a PVC size that is available, or that your PVC storage class is set up correctly.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="pvcs-do-not-resize"><a class="anchor" href="#pvcs-do-not-resize"></a>6.3.2. PVCs Do Not Resize</h4>
<div class="paragraph">
<p>Ensure that your storage class supports PVC resizing. You can check that by inspecting the <code>allowVolumeExpansion</code> attribute:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get sc</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the storage class does not support PVC resizing, you can use the technique described above to resize PVCs using a second instance set.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-4"><a class="anchor" href="#next-steps-4"></a>6.4. Next Steps</h3>
<div class="paragraph">
<p>You&#8217;ve now resized your Ivory cluster, but how can you configure Ivory to take advantage of the new resources? Let&#8217;s look at how we can <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md">customize the Ivory cluster configuration</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="custom-ivory-configuration"><a class="anchor" href="#custom-ivory-configuration"></a>7. Custom Ivory Configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Part of the trick of managing multiple instances in an Ivory cluster is ensuring all of the configuration
changes are propagated to each of them. This is where IVYO helps: when you make an Ivory configuration
change for a cluster, IVYO will apply it to all of the Ivory instances.</p>
</div>
<div class="paragraph">
<p>For example, in our previous step we added CPU and memory limits of <code>2.0</code> and <code>4Gi</code> respectively. Let&#8217;s tweak some of the Ivory settings to better use our new resources. We can do this in the <code>spec.patroni.dynamicConfiguration</code> section. Here is an example updated manifest that tweaks several settings:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - name: instance1
      replicas: 2
      resources:
        limits:
          cpu: 2.0
          memory: 4Gi
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi
  patroni:
    dynamicConfiguration:
      postgresql:
        parameters:
          max_parallel_workers: 2
          max_worker_processes: 2
          shared_buffers: 1GB
          work_mem: 2MB</code></pre>
</div>
</div>
<div class="paragraph">
<p>In particular, we added the following to <code>spec</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">patroni:
  dynamicConfiguration:
    postgresql:
      parameters:
        max_parallel_workers: 2
        max_worker_processes: 2
        shared_buffers: 1GB
        work_mem: 2MB</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply these updates to your Ivory cluster with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>IVYO will go and apply these settings, restarting each Ivory instance when necessary. You can verify that the changes are present using the Ivory <code>SHOW</code> command, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">SHOW work_mem;</code></pre>
</div>
</div>
<div class="paragraph">
<p>should yield something similar to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"> work_mem
----------
 2MB</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="customize-tls"><a class="anchor" href="#customize-tls"></a>7.1. Customize TLS</h3>
<div class="paragraph">
<p>All connections in IVYO use TLS to encrypt communication between components. IVYO sets up a PKI and certificate authority (CA) that allow you create verifiable endpoints. However, you may want to bring a different TLS infrastructure based upon your organizational requirements. The good news: IVYO lets you do this!</p>
</div>
<div class="sect3">
<h4 id="how-to-customize-tls"><a class="anchor" href="#how-to-customize-tls"></a>7.1.1. How to Customize TLS</h4>
<div class="paragraph">
<p>There are a few different TLS endpoints that can be customized for IVYO, including those of the Ivory cluster and controlling how Ivory instances authenticate with each other. Let&#8217;s look at how we can customize TLS by defining</p>
</div>
<div class="ulist">
<ul>
<li>
<p>a <code>spec.customTLSSecret</code>, used to both identify the cluster and encrypt communications; and</p>
</li>
<li>
<p>a <code>spec.customReplicationTLSSecret</code>, used for replication authentication.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To customize the TLS for an Ivory cluster, you will need to create two Secrets in the Namespace of your Ivory cluster. One of these Secrets will be the <code>customTLSSecret</code> and the other will be the <code>customReplicationTLSSecret</code>. Both secrets contain a TLS key (<code>tls.key</code>), TLS certificate (<code>tls.crt</code>) and CA certificate (<code>ca.crt</code>) to use.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If <code>spec.customTLSSecret</code> is provided you <strong>must</strong> also provide <code>spec.customReplicationTLSSecret</code> and both must contain the same <code>ca.crt</code>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The custom TLS and custom replication TLS Secrets should contain the following fields (though see below for a workaround if you cannot control the field names of the Secret&#8217;s <code>data</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">data:
  ca.crt: &lt;value&gt;
  tls.crt: &lt;value&gt;
  tls.key: &lt;value&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, if you have files named <code>ca.crt</code>, <code>hippo.key</code>, and <code>hippo.crt</code> stored on your local machine, you could run the following command to create a Secret from those files:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create secret generic -n ivory-operator hippo-cluster.tls \
  --from-file=ca.crt=ca.crt \
  --from-file=tls.key=hippo.key \
  --from-file=tls.crt=hippo.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>After you create the Secrets, you can specify the custom TLS Secret in your <code>ivorycluster.ivory-operator.ivorysql.org</code> custom resource. For example, if you created a <code>hippo-cluster.tls</code> Secret and a <code>hippo-replication.tls</code> Secret, you would add them to your Ivory cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  customTLSSecret:
    name: hippo-cluster.tls
  customReplicationTLSSecret:
    name: hippo-replication.tls</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you&#8217;re unable to control the key-value pairs in the Secret, you can create a mapping to tell
the Ivory Operator what key holds the expected value. That would look similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  customTLSSecret:
    name: hippo.tls
    items:
      - key: &lt;tls.crt key in the referenced hippo.tls Secret&gt;
        path: tls.crt
      - key: &lt;tls.key key in the referenced hippo.tls Secret&gt;
        path: tls.key
      - key: &lt;ca.crt key in the referenced hippo.tls Secret&gt;
        path: ca.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>For instance, if the <code>hippo.tls</code> Secret had the <code>tls.crt</code> in a key named <code>hippo-tls.crt</code>, the
<code>tls.key</code> in a key named <code>hippo-tls.key</code>, and the <code>ca.crt</code> in a key named <code>hippo-ca.crt</code>,
then your mapping would look like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  customTLSSecret:
    name: hippo.tls
    items:
      - key: hippo-tls.crt
        path: tls.crt
      - key: hippo-tls.key
        path: tls.key
      - key: hippo-ca.crt
        path: ca.crt</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Although the custom TLS and custom replication TLS Secrets share the same <code>ca.crt</code>, they do not share the same <code>tls.crt</code>:
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Your <code>spec.customTLSSecret</code> TLS certificate should have a Common Name (CN) setting that matches the primary Service name. This is the name of the cluster suffixed with <code>-primary</code>. For example, for our <code>hippo</code> cluster this would be <code>hippo-primary</code>.</p>
</li>
<li>
<p>Your <code>spec.customReplicationTLSSecret</code> TLS certificate should have a Common Name (CN) setting that matches <code>_ivoryrepl</code>, which is the preset replication user.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As with the other changes, you can roll out the TLS customizations with <code>kubectl apply</code>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="labels"><a class="anchor" href="#labels"></a>7.2. Labels</h3>
<div class="paragraph">
<p>There are several ways to add your own custom Kubernetes <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels</a> to your Ivory cluster.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Cluster: You can apply labels to any IVYO managed object in a cluster by editing the <code>spec.metadata.labels</code> section of the custom resource.</p>
</li>
<li>
<p>Ivory: You can apply labels to an Ivory instance set and its objects by editing <code>spec.instances.metadata.labels</code>.</p>
</li>
<li>
<p>pgBackRest: You can apply labels to pgBackRest and its objects by editing <code>ivoryclusters.spec.backups.pgbackrest.metadata.labels</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="annotations"><a class="anchor" href="#annotations"></a>7.3. Annotations</h3>
<div class="paragraph">
<p>There are several ways to add your own custom Kubernetes <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a> to your Ivory cluster.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Cluster: You can apply annotations to any IVYO managed object in a cluster by editing the <code>spec.metadata.annotations</code> section of the custom resource.</p>
</li>
<li>
<p>Ivory: You can apply annotations to an Ivory instance set and its objects by editing <code>spec.instances.metadata.annotations</code>.</p>
</li>
<li>
<p>pgBackRest: You can apply annotations to pgBackRest and its objects by editing <code>spec.backups.pgbackrest.metadata.annotations</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="pod-priority-classes"><a class="anchor" href="#pod-priority-classes"></a>7.4. Pod Priority Classes</h3>
<div class="paragraph">
<p>IVYO allows you to use <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/">pod priority classes</a> to indicate the relative importance of a pod by setting a <code>priorityClassName</code> field on your Ivory cluster. This can be done as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Instances: Priority is defined per instance set and is applied to all Pods in that instance set by editing the <code>spec.instances.priorityClassName</code> section of the custom resource.</p>
</li>
<li>
<p>Dedicated Repo Host: Priority defined under the repoHost section of the spec is applied to the dedicated repo host by editing the <code>spec.backups.pgbackrest.repoHost.priorityClassName</code> section of the custom resource.</p>
</li>
<li>
<p>Backup (manual and scheduled): Priority is defined under the <code>spec.backups.pgbackrest.jobs.priorityClassName</code> section and applies that priority to all pgBackRest backup Jobs (manual and scheduled).</p>
</li>
<li>
<p>Restore (data source or in-place): Priority is defined for either a "data source" restore or an in-place restore by editing the <code>spec.dataSource.ivorycluster.priorityClassName</code> section of the custom resource.</p>
</li>
<li>
<p>Data Migration: The priority defined for the first instance set in the spec (array position 0) is used for the PGDATA and WAL migration Jobs. The pgBackRest repo migration Job will use the priority class applied to the repoHost.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="separate-wal-pvcs"><a class="anchor" href="#separate-wal-pvcs"></a>7.5. Separate WAL PVCs</h3>
<div class="paragraph">
<p>IvorySQL commits transactions by storing changes in its <a href="https://www.postgresql.org/docs/current/wal-intro.html">Write-Ahead Log (WAL)</a>. Because the way WAL files are accessed and
utilized often differs from that of data files, and in high-performance situations, it can desirable to put WAL files on separate storage volume. With IVYO, this can be done by adding
the <code>walVolumeClaimSpec</code> block to your desired instance in your ivorycluster spec, either when your cluster is created or anytime thereafter:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  instances:
    - name: instance
      walVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>This volume can be removed later by removing the <code>walVolumeClaimSpec</code> section from the instance. Note that when changing the WAL directory, care is taken so as not to lose any WAL files. IVYO only
deletes the PVC once there are no longer any WAL files on the previously configured volume.</p>
</div>
</div>
<div class="sect2">
<h3 id="database-initialization-sql"><a class="anchor" href="#database-initialization-sql"></a>7.6. Database Initialization SQL</h3>
<div class="paragraph">
<p>IVYO can run SQL for you as part of the cluster creation and initialization process. IVYO runs the SQL using the psql client so you can use meta-commands to connect to different databases, change error handling, or set and use variables. Its capabilities are described in the <a href="https://www.postgresql.org/docs/current/app-psql.html">psql documentation</a>.</p>
</div>
<div class="sect3">
<h4 id="initialization-sql-configmap"><a class="anchor" href="#initialization-sql-configmap"></a>7.6.1. Initialization SQL ConfigMap</h4>
<div class="paragraph">
<p>The Ivory cluster spec accepts a reference to a ConfigMap containing your init SQL file. Update your cluster spec to include the ConfigMap name, <code>spec.databaseInitSQL.name</code>, and the data key, <code>spec.databaseInitSQL.key</code>, for your SQL file. For example, if you create your ConfigMap with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator create configmap hippo-init-sql --from-file=init.sql=/path/to/init.sql</code></pre>
</div>
</div>
<div class="paragraph">
<p>You would add the following section to your ivorycluster spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  databaseInitSQL:
    key: init.sql
    name: hippo-init-sql</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The ConfigMap must exist in the same namespace as your Ivory cluster.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After you add the ConfigMap reference to your spec, apply the change with <code>kubectl apply -k examples/kustomize/ivory</code>. IVYO will create your <code>hippo</code> cluster and run your initialization SQL once the cluster has started. You can verify that your SQL has been run by checking the <code>databaseInitSQL</code> status on your Ivory cluster. While the status is set, your init SQL will not be run again. You can check cluster status with the <code>kubectl describe</code> command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator describe ivoryclusters.ivory-operator.ivorysql.org hippo</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
In some cases, due to how Kubernetes treats ivorycluster status, IVYO may run your SQL commands more than once. Please ensure that the commands defined in your init SQL are idempotent.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now that <code>databaseInitSQL</code> is defined in your cluster status, verify database objects have been created as expected. After verifying, we recommend removing the <code>spec.databaseInitSQL</code> field from your spec. Removing the field from the spec will also remove <code>databaseInitSQL</code> from the cluster status.</p>
</div>
</div>
<div class="sect3">
<h4 id="psql-usage"><a class="anchor" href="#psql-usage"></a>7.6.2. PSQL Usage</h4>
<div class="paragraph">
<p>IVYO uses the psql interactive terminal to execute SQL statements in your database. Statements are passed in using standard input and the filename flag (e.g. <code>psql -f -</code>).</p>
</div>
<div class="paragraph">
<p>SQL statements are executed as superuser in the default maintenance database. This means you have full control to create database objects, extensions, or run any SQL statements that you might need.</p>
</div>
<div class="sect4">
<h5 id="integration-with-user-and-database-management"><a class="anchor" href="#integration-with-user-and-database-management"></a>7.6.2.1. Integration with User and Database Management</h5>
<div class="paragraph">
<p>If you are creating users or databases, please see the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/user-management.md">User/Database Management</a> documentation. Databases created through the user management section of the spec can be referenced in your initialization sql. For example, if a database <code>zoo</code> is defined:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: hippo
      databases:
       - "zoo"</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can connect to <code>zoo</code> by adding the following <code>psql</code> meta-command to your SQL:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sql hljs" data-lang="sql">\c zoo
create table t_zoo as select s, md5(random()::text) from generate_Series(1,5) s;</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="transaction-support"><a class="anchor" href="#transaction-support"></a>7.6.2.2. Transaction support</h5>
<div class="paragraph">
<p>By default, <code>psql</code> commits each SQL command as it completes. To combine multiple commands into a single <a href="https://www.postgresql.org/docs/current/tutorial-transactions.html">transaction</a>, use the <a href="https://www.postgresql.org/docs/current/sql-begin.html"><code>BEGIN</code></a> and <a href="https://www.postgresql.org/docs/current/sql-commit.html"><code>COMMIT</code></a> commands.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sql hljs" data-lang="sql">BEGIN;
create table t_random as select s, md5(random()::text) from generate_Series(1,5) s;
COMMIT;</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="psql-exit-code-and-database-init-sql-status"><a class="anchor" href="#psql-exit-code-and-database-init-sql-status"></a>7.6.2.3. PSQL Exit Code and Database Init SQL Status</h5>
<div class="paragraph">
<p>The exit code from <code>psql</code> will determine when the <code>databaseInitSQL</code> status is set. When <code>psql</code> returns <code>0</code> the status will be set and SQL will not be run again. When <code>psql</code> returns with an error exit code the status will not be set. IVYO will continue attempting to execute the SQL as part of its reconcile loop until <code>psql</code> returns normally. If <code>psql</code> exits with a failure, you will need to edit the file in your ConfigMap to ensure your SQL statements will lead to a successful <code>psql</code> return. The easiest way to make live changes to your ConfigMap is to use the following <code>kubectl edit</code> command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n &lt;cluster-namespace&gt; edit configmap hippo-init-sql</code></pre>
</div>
</div>
<div class="paragraph">
<p>Be sure to transfer any changes back over to your local file. Another option is to make changes in your local file and use <code>kubectl --dry-run</code> to create a template and pipe the output into <code>kubectl apply</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create configmap hippo-init-sql --from-file=init.sql=/path/to/init.sql --dry-run=client -o yaml | kubectl apply -f -</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
If you edit your ConfigMap and your changes aren&#8217;t showing up, you may be waiting for IVYO to reconcile your cluster. After some time, IVYO will automatically reconcile the cluster or you can trigger reconciliation by applying any change to your cluster (e.g. with <code>kubectl apply -k examples/kustomize/ivory</code>).
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To ensure that <code>psql</code> returns a failure exit code when your SQL commands fail, set the <code>ON_ERROR_STOP</code> <a href="https://www.postgresql.org/docs/current/app-psql.html#APP-PSQL-VARIABLES">variable</a> as part of your SQL file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sql hljs" data-lang="sql">\set ON_ERROR_STOP
\echo Any error will lead to exit code 3
create table t_random as select s, md5(random()::text) from generate_Series(1,5) s;</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting-3"><a class="anchor" href="#troubleshooting-3"></a>7.7. Troubleshooting</h3>
<div class="sect3">
<h4 id="changes-not-applied"><a class="anchor" href="#changes-not-applied"></a>7.7.1. Changes Not Applied</h4>
<div class="paragraph">
<p>If your Ivory configuration settings are not present, ensure that you are using the syntax that Ivory expects.
You can see this in the <a href="https://www.postgresql.org/docs/current/runtime-config.html">Ivory configuration documentation</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-5"><a class="anchor" href="#next-steps-5"></a>7.8. Next Steps</h3>
<div class="paragraph">
<p>You&#8217;ve now seen how you can further customize your Ivory cluster, but what about <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/user-management.md">managing users and databases</a>? That&#8217;s a great question that is answered in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/user-management.md">next section</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="user-database-management"><a class="anchor" href="#user-database-management"></a>8. User / Database Management</h2>
<div class="sectionbody">
<div class="paragraph">
<p>IVYO comes with some out-of-the-box conveniences for managing users and databases in your Ivory cluster. However, you may have requirements where you need to create additional users, adjust user privileges or add additional databases to your cluster.</p>
</div>
<div class="paragraph">
<p>For detailed information for how user and database management works in IVYO, please see the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/architecture/user-management.md">User Management</a> section of the architecture guide.</p>
</div>
<div class="sect2">
<h3 id="creating-a-new-user"><a class="anchor" href="#creating-a-new-user"></a>8.1. Creating a New User</h3>
<div class="paragraph">
<p>You can create a new user with the following snippet in the <code>ivorycluster</code> custom resource. Let&#8217;s add this to our <code>hippo</code> database:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: rhino</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can now apply the changes and see that the new user is created. Note the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The user would only be able to connect to the default <code>ivory</code> database.</p>
</li>
<li>
<p>The user will not have any connection credentials populated into the <code>hippo-pguser-rhino</code> Secret.</p>
</li>
<li>
<p>The user is unprivileged.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s create a new database named <code>zoo</code> that we will let the <code>rhino</code> user access:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: rhino
      databases:
        - zoo</code></pre>
</div>
</div>
<div class="paragraph">
<p>Inspect the <code>hippo-pguser-rhino</code> Secret. You should now see that the <code>dbname</code> and <code>uri</code> fields are now populated!</p>
</div>
<div class="paragraph">
<p>We can set role privileges by using the standard <a href="https://www.postgresql.org/docs/current/role-attributes.html">role attributes</a> that Ivory provides and adding them to the <code>spec.users.options</code>. Let&#8217;s say we want the rhino to become a superuser (be careful about doling out Ivory superuser privileges!). You can add the following to the spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: rhino
      databases:
        - zoo
      options: "SUPERUSER"</code></pre>
</div>
</div>
<div class="paragraph">
<p>There you have it: we have created a Ivory user named <code>rhino</code> with superuser privileges that has access to the <code>rhino</code> database (though a superuser has access to all databases!).</p>
</div>
</div>
<div class="sect2">
<h3 id="adjusting-privileges"><a class="anchor" href="#adjusting-privileges"></a>8.2. Adjusting Privileges</h3>
<div class="paragraph">
<p>Let&#8217;s say you want to revoke the superuser privilege from <code>rhino</code>. You can do so with the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: rhino
      databases:
        - zoo
      options: "NOSUPERUSER"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you want to add multiple privileges, you can add each privilege with a space between them in <code>options</code>, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: rhino
      databases:
        - zoo
      options: "CREATEDB CREATEROLE"</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="managing-the-ivory-user"><a class="anchor" href="#managing-the-ivory-user"></a>8.3. Managing the <code>ivory</code> User</h3>
<div class="paragraph">
<p>By default, IVYO does not give you access to the <code>ivory</code> user. However, you can get access to this account by doing the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  users:
    - name: ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will create a Secret of the pattern <code>&lt;clusterName&gt;-pguser-ivory</code> that contains the credentials of the <code>ivory</code> account. For our <code>hippo</code> cluster, this would be <code>hippo-pguser-ivory</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="deleting-a-user"><a class="anchor" href="#deleting-a-user"></a>8.4. Deleting a User</h3>
<div class="paragraph">
<p>IVYO does not delete users automatically: after you remove the user from the spec, it will still exist in your cluster. To remove a user and all of its objects, as a superuser you will need to run <a href="https://www.postgresql.org/docs/current/sql-drop-owned.html"><code>DROP OWNED</code></a> in each database the user has objects in, and <a href="https://www.postgresql.org/docs/current/sql-droprole.html"><code>DROP ROLE</code></a>
in your Ivory cluster.</p>
</div>
<div class="paragraph">
<p>For example, with the above <code>rhino</code> user, you would run the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sql hljs" data-lang="sql">DROP OWNED BY rhino;
DROP ROLE rhino;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that you may need to run <code>DROP OWNED BY rhino CASCADE;</code> based upon your object ownership structure&#8201;&#8212;&#8201;be very careful with this command!</p>
</div>
</div>
<div class="sect2">
<h3 id="deleting-a-database"><a class="anchor" href="#deleting-a-database"></a>8.5. Deleting a Database</h3>
<div class="paragraph">
<p>IVYO does not delete databases automatically: after you remove all instances of the database from the spec, it will still exist in your cluster. To completely remove the database, you must run the <a href="https://www.postgresql.org/docs/current/sql-dropdatabase.html"><code>DROP DATABASE</code></a>
command as a Ivory superuser.</p>
</div>
<div class="paragraph">
<p>For example, to remove the <code>zoo</code> database, you would execute the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sql hljs" data-lang="sql">DROP DATABASE zoo;</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-6"><a class="anchor" href="#next-steps-6"></a>8.6. Next Steps</h3>
<div class="paragraph">
<p>Let&#8217;s look at how IVYO handles <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/disaster-recovery.md">disaster recovery</a>!</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="disaster-recovery-and-cloning"><a class="anchor" href="#disaster-recovery-and-cloning"></a>9. Disaster Recovery and Cloning</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Perhaps someone accidentally dropped the <code>users</code> table. Perhaps you want to clone your production database to a step-down environment. Perhaps you want to exercise your disaster recovery system (and it is important that you do!).</p>
</div>
<div class="paragraph">
<p>Regardless of scenario, it&#8217;s important to know how you can perform a "restore" operation with IVYO to be able to recovery your data from a particular point in time, or clone a database for other purposes.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s look at how we can perform different types of restore operations. First, let&#8217;s understand the core restore properties on the custom resource.</p>
</div>
<div class="sect2">
<h3 id="restore-properties"><a class="anchor" href="#restore-properties"></a>9.1. Restore Properties</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>IVYO offers the ability to restore from an existing ivorycluster or a remote
cloud-based data source, such as S3, GCS, etc. For more on that, see the <a href="#cloud-based-data-source">Clone From Backups Stored in S3 / GCS / Azure Blob Storage</a> section.</p>
</div>
<div class="paragraph">
<p>Note that you <strong>cannot</strong> use both a local ivorycluster data source and a remote cloud-based data
source at one time; if both the <code>dataSource.ivorycluster</code> and <code>dataSource.pgbackrest</code> fields
are filled in, the local ivorycluster data source will take precedence.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>There are several attributes on the custom resource that are important to understand as part of the restore process. All of these attributes are grouped together in the spec.dataSource.ivorycluster section of the custom resource.</p>
</div>
<div class="paragraph">
<p>Please review the table below to understand how each of these attributes work in the context of setting up a restore operation.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.dataSource.ivorycluster.clusterName</code>: The name of the cluster that you are restoring from. This corresponds to the <code>metadata.name</code> attribute on a different <code>ivorycluster</code> custom resource.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.clusterNamespace</code>: The namespace of the cluster that you are restoring from. Used when the cluster exists in a different namespace.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.repoName</code>: The name of the pgBackRest repository from the <code>spec.dataSource.ivorycluster.clusterName</code> to use for the restore. Can be one of <code>repo1</code>, <code>repo2</code>, <code>repo3</code>, or <code>repo4</code>. The repository must exist in the other cluster.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.options</code>: Any additional <a href="https://pgbackrest.org/command.html#command-restore">pgBackRest restore options</a> or general options that IVYO allows. For example, you may want to set <code>--process-max</code> to help improve performance on larger databases; but you will not be able to set`--target-action`, since that option is currently disallowed. (IVYO always sets it to <code>promote</code> if a <code>--target</code> is present, and otherwise leaves it blank.)</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.resources</code>: Setting <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">resource limits and requests</a> of the restore job can ensure that it runs efficiently.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.affinity</code>: Custom <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Kubernetes affinity</a> rules constrain the restore job so that it only runs on certain nodes.</p>
</li>
<li>
<p><code>spec.dataSource.ivorycluster.tolerations</code>: Custom <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Kubernetes tolerations</a> allow the restore job to run on <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tainted</a> nodes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s walk through some examples for how we can clone and restore our databases.</p>
</div>
</div>
<div class="sect2">
<h3 id="clone-a-ivory-cluster"><a class="anchor" href="#clone-a-ivory-cluster"></a>9.2. Clone a Ivory Cluster</h3>
<div class="paragraph">
<p>Let&#8217;s create a clone of our <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/create-cluster.md"><code>hippo</code></a> cluster that we created previously. We know that our cluster is named <code>hippo</code> (based on its <code>metadata.name</code>) and that we only have a single backup repository called <code>repo1</code>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s call our new cluster <code>elephant</code>. We can create a clone of the <code>hippo</code> cluster using a manifest like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: elephant
spec:
  dataSource:
    ivoryCluster:
      clusterName: hippo
      repoName: repo1
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note this section of the spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  dataSource:
    ivoryCluster:
      clusterName: hippo
      repoName: repo1</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is the part that tells IVYO to create the <code>elephant</code> cluster as an independent copy of the <code>hippo</code> cluster.</p>
</div>
<div class="paragraph">
<p>The above is all you need to do to clone a Ivory cluster! IVYO will work on creating a copy of your data on a new persistent volume claim (PVC) and work on initializing your cluster to spec. Easy!</p>
</div>
</div>
<div class="sect2">
<h3 id="perform-a-point-in-time-recovery-pitr"><a class="anchor" href="#perform-a-point-in-time-recovery-pitr"></a>9.3. Perform a Point-in-time-Recovery (PITR)</h3>
<div class="paragraph">
<p>Did someone drop the user table? You may want to perform a point-in-time-recovery (PITR)
to revert your database back to a state before a change occurred. Fortunately, IVYO can help you do that.</p>
</div>
<div class="paragraph">
<p>You can set up a PITR using the <a href="https://pgbackrest.org/command.html#command-restore">restore</a>
command of <a href="https://www.pgbackrest.org">pgBackRest</a>, the backup management tool that powers
the disaster recovery capabilities of IVYO. You will need to set a few options on
<code>spec.dataSource.ivorycluster.options</code> to perform a PITR. These options include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>--type=time</code>: This tells pgBackRest to perform a PITR.</p>
</li>
<li>
<p><code>--target</code>: Where to perform the PITR to. An example recovery target is <code>2021-06-09 14:15:11-04</code>.
The timezone specified here as -04 for EDT. Please see the <a href="https://pgbackrest.org/user-guide.html#pitr">pgBackRest documentation for other timezone options</a>.</p>
</li>
<li>
<p><code>--set</code> (optional): Choose which backup to start the PITR from.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A few quick notes before we begin:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To perform a PITR, you must have a backup that finished before your PITR time.
In other words, you can&#8217;t perform a PITR back to a time where you do not have a backup!</p>
</li>
<li>
<p>All relevant WAL files must be successfully pushed for the restore to complete correctly.</p>
</li>
<li>
<p>Be sure to select the correct repository name containing the desired backup!</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>With that in mind, let&#8217;s use the <code>elephant</code> example above. Let&#8217;s say we want to perform a point-in-time-recovery (PITR) to <code>2021-06-09 14:15:11-04</code>, we can use the following manifest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: elephant
spec:
  dataSource:
    ivoryCluster:
      clusterName: hippo
      repoName: repo1
      options:
      - --type=time
      - --target="2021-06-09 14:15:11-04"
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
<div class="paragraph">
<p>The section to pay attention to is this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  dataSource:
    ivoryCluster:
      clusterName: hippo
      repoName: repo1
      options:
      - --type=time
      - --target="2021-06-09 14:15:11-04"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice how we put in the options to specify where to make the PITR.</p>
</div>
<div class="paragraph">
<p>Using the above manifest, IVYO will go ahead and create a new Ivory cluster that recovers
its data up until <code>2021-06-09 14:15:11-04</code>. At that point, the cluster is promoted and
you can start accessing your database from that specific point in time!</p>
</div>
</div>
<div class="sect2">
<h3 id="perform-an-in-place-point-in-time-recovery-pitr"><a class="anchor" href="#perform-an-in-place-point-in-time-recovery-pitr"></a>9.4. Perform an In-Place Point-in-time-Recovery (PITR)</h3>
<div class="paragraph">
<p>Similar to the PITR restore described above, you may want to perform a similar reversion
back to a state before a change occurred, but without creating another IvorySQL cluster.
Fortunately, IVYO can help you do this as well.</p>
</div>
<div class="paragraph">
<p>You can set up a PITR using the <a href="https://pgbackrest.org/command.html#command-restore">restore</a>
command of <a href="https://www.pgbackrest.org">pgBackRest</a>, the backup management tool that powers
the disaster recovery capabilities of IVYO. You will need to set a few options on
<code>spec.backups.pgbackrest.restore.options</code> to perform a PITR. These options include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>--type=time</code>: This tells pgBackRest to perform a PITR.</p>
</li>
<li>
<p><code>--target</code>: Where to perform the PITR to. An example recovery target is <code>2021-06-09 14:15:11-04</code>.</p>
</li>
<li>
<p><code>--set</code> (optional): Choose which backup to start the PITR from.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A few quick notes before we begin:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To perform a PITR, you must have a backup that finished before your PITR time.
In other words, you can&#8217;t perform a PITR back to a time where you do not have a backup!</p>
</li>
<li>
<p>All relevant WAL files must be successfully pushed for the restore to complete correctly.</p>
</li>
<li>
<p>Be sure to select the correct repository name containing the desired backup!</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To perform an in-place restore, users will first fill out the restore section of the spec as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  backups:
    pgbackrest:
      restore:
        enabled: true
        repoName: repo1
        options:
        - --type=time
        - --target="2021-06-09 14:15:11-04"</code></pre>
</div>
</div>
<div class="paragraph">
<p>And to trigger the restore, you will then annotate the ivorycluster as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl annotate -n ivory-operator ivorycluster hippo --overwrite \
  ivory-operator.ivorysql.org/pgbackrest-restore=id1</code></pre>
</div>
</div>
<div class="paragraph">
<p>And once the restore is complete, in-place restores can be disabled:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  backups:
    pgbackrest:
      restore:
        enabled: false</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice how we put in the options to specify where to make the PITR.</p>
</div>
<div class="paragraph">
<p>Using the above manifest, IVYO will go ahead and re-create your Ivory cluster to recover
its data up until <code>2021-06-09 14:15:11-04</code>. At that point, the cluster is promoted and
you can start accessing your database from that specific point in time!</p>
</div>
</div>
<div class="sect2">
<h3 id="restore-individual-databases"><a class="anchor" href="#restore-individual-databases"></a>9.5. Restore Individual Databases</h3>
<div class="paragraph">
<p>You might need to restore specific databases from a cluster backup, for performance reasons
or to move selected databases to a machine that does not have enough space to restore the
entire cluster backup.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>pgBackRest supports this case, but it is important to make sure this is what you want.
Restoring in this manner will restore the requested database from backup and make it
accessible, but all of the other databases in the backup will NOT be accessible after restore.</p>
</div>
<div class="paragraph">
<p>For example, if your backup includes databases <code>test1</code>, <code>test2</code>, and <code>test3</code>, and you request that
<code>test2</code> be restored, the <code>test1</code> and <code>test3</code> databases will NOT be accessible after restore is completed.
Please review the pgBackRest documentation on the
<a href="https://pgbackrest.org/user-guide.html#restore/option-db-include">limitations on restoring individual databases</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can restore individual databases from a backup using a spec similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  backups:
    pgbackrest:
      restore:
        enabled: true
        repoName: repo1
        options:
        - --db-include=hippo</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>--db-include=hippo</code> would restore only the contents of the <code>hippo</code> database.</p>
</div>
</div>
<div class="sect2">
<h3 id="standby-cluster"><a class="anchor" href="#standby-cluster"></a>9.6. Standby Cluster</h3>
<div class="paragraph">
<p>Advanced high-availability and disaster recovery strategies involve spreading your database clusters
across data centers to help maximize uptime. IVYO provides ways to deploy ivoryclusters that can
span multiple Kubernetes clusters using an external storage system or IvorySQL streaming replication.
The <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/architecture/disaster-recovery.md">disaster recovery architecture</a> documentation
provides a high-level overview of standby clusters with IVYO can be found in the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/architecture/disaster-recovery.md">disaster recovery
architecture</a> documentation.</p>
</div>
<div class="sect3">
<h4 id="creating-a-standby-cluster"><a class="anchor" href="#creating-a-standby-cluster"></a>9.6.1. Creating a standby Cluster</h4>
<div class="paragraph">
<p>This tutorial section will describe how to create three different types of standby clusters, one
using an external storage system, one that is streaming data directly from the primary, and one that
takes advantage of both external storage and streaming. These example clusters can be created in the
same Kubernetes cluster, using a single IVYO instance, or spread across different Kubernetes clusters
and IVYO instances with the correct storage and networking configurations.</p>
</div>
<div class="sect4">
<h5 id="repo-based-standby"><a class="anchor" href="#repo-based-standby"></a>9.6.1.1. Repo-based Standby</h5>
<div class="paragraph">
<p>A repo-based standby will recover from WAL files a pgBackRest repo stored in external storage. The
primary cluster should be created with a cloud-based <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/backups.md">backup configuration</a>. The following manifest defines a ivorycluster with <code>standby.enabled</code> set to true and <code>repoName</code>
configured to point to the <code>s3</code> repo configured in the primary:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo-standby
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec: { accessModes: [ReadWriteOnce], resources: { requests: { storage: 1Gi } } }
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"
  standby:
    enabled: true
    repoName: repo1</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="streaming-standby"><a class="anchor" href="#streaming-standby"></a>9.6.1.2. Streaming Standby</h5>
<div class="paragraph">
<p>A streaming standby relies on an authenticated connection to the primary over the network. The primary
cluster should be accessible via the network and allow TLS authentication (TLS is enabled by default).
In the following manifest, we have <code>standby.enabled</code> set to <code>true</code> and have provided both the <code>host</code>
and <code>port</code> that point to the primary cluster. We have also defined <code>customTLSSecret</code> and
<code>customReplicationTLSSecret</code> to provide certs that allow the standby to authenticate to the primary.
For this type of standby, you must use <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md#customize-tls">custom TLS</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo-standby
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec: { accessModes: [ReadWriteOnce], resources: { requests: { storage: 1Gi } } }
  backups:
    pgbackrest:
      repos:
      - name: repo1
        volume:
          volumeClaimSpec: { accessModes: [ReadWriteOnce], resources: { requests: { storage: 1Gi } } }
  customTLSSecret:
    name: cluster-cert
  customReplicationTLSSecret:
    name: replication-cert
  standby:
    enabled: true
    host: "192.0.2.2"
    port: 5432</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="streaming-standby-with-an-external-repo"><a class="anchor" href="#streaming-standby-with-an-external-repo"></a>9.6.1.3. Streaming Standby with an External Repo</h5>
<div class="paragraph">
<p>Another option is to create a standby cluster using an external pgBackRest repo that streams from the
primary. With this setup, the standby cluster will continue recovering from the pgBackRest repo if
streaming replication falls behind. In this manifest, we have enabled the settings from both previous
examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo-standby
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec: { accessModes: [ReadWriteOnce], resources: { requests: { storage: 1Gi } } }
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"
  customTLSSecret:
    name: cluster-cert
  customReplicationTLSSecret:
    name: replication-cert
  standby:
    enabled: true
    repoName: repo1
    host: "192.0.2.2"
    port: 5432</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="promoting-a-standby-cluster"><a class="anchor" href="#promoting-a-standby-cluster"></a>9.7. Promoting a Standby Cluster</h3>
<div class="paragraph">
<p>At some point, you will want to promote the standby to start accepting both reads and writes.
This has the net effect of pushing WAL (transaction archives) to the pgBackRest repository, so we
need to ensure we don&#8217;t accidentally create a split-brain scenario. Split-brain can happen if two
primary instances attempt to write to the same repository. If the primary cluster is still active,
make sure you <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/administrative-tasks.md#shutdown">shutdown</a> the primary
before trying to promote the standby cluster.</p>
</div>
<div class="paragraph">
<p>Once the primary is inactive, we can promote the standby cluster by removing or disabling its
<code>spec.standby</code> section:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  standby:
    enabled: false</code></pre>
</div>
</div>
<div class="paragraph">
<p>This change triggers the promotion of the standby leader to a primary IvorySQL
instance and the cluster begins accepting writes.</p>
</div>
</div>
<div class="sect2">
<h3 id="cloud-based-data-source"><a class="anchor" href="#cloud-based-data-source"></a>9.8. Clone From Backups Stored in S3 / GCS / Azure Blob Storage</h3>
<div class="paragraph">
<p>You can clone a Ivory cluster from backups that are stored in AWS S3 (or a storage system
that uses the S3 protocol), GCS, or Azure Blob Storage without needing an active Ivory cluster!
The method to do so is similar to how you clone from an existing ivorycluster. This is useful
if you want to have a data set for people to use but keep it compressed on cheaper storage.</p>
</div>
<div class="paragraph">
<p>For the purposes of this example, let&#8217;s say that you created a Ivory cluster named <code>hippo</code> that
has its backups stored in S3 that looks similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: hippo
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  instances:
    - dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      configuration:
      - secret:
          name: ivyo-s3-creds
      global:
        repo1-path: /pgbackrest/ivory-operator/hippo/repo1
      manual:
        repoName: repo1
        options:
         - --type=full
      repos:
      - name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Ensure that the credentials in <code>ivyo-s3-creds</code> match your S3 credentials. For more details on
<a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/backups.md#using-s3">deploying a Ivory cluster using S3 for backups</a>,
please see the <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/backups.md#using-s3">Backups</a> section of the tutorial.</p>
</div>
<div class="paragraph">
<p>For optimal performance when creating a new cluster from an active cluster, ensure that you take a
recent full backup of the previous cluster. The above manifest is set up to take a full backup.
Assuming <code>hippo</code> is created in the <code>ivory-operator</code> namespace, you can trigger a full backup
with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl annotate -n ivory-operator ivorycluster hippo --overwrite \
  ivory-operator.ivorysql.org/pgbackrest-backup="$( date '+%F_%H:%M:%S' )"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the backup to complete. Once this is done, you can delete the Ivory cluster.</p>
</div>
<div class="paragraph">
<p>Now, let&#8217;s clone the data from the <code>hippo</code> backup into a new cluster called <code>elephant</code>. You can use a manifest similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: elephant
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  dataSource:
    pgbackrest:
      stanza: db
      configuration:
      - secret:
          name: ivyo-s3-creds
      global:
        repo1-path: /pgbackrest/ivory-operator/hippo/repo1
      repo:
        name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"
  instances:
    - dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      configuration:
      - secret:
          name: ivyo-s3-creds
      global:
        repo1-path: /pgbackrest/ivory-operator/elephant/repo1
      repos:
      - name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are a few things to note in this manifest. First, note that the <code>spec.dataSource.pgbackrest</code>
object in our new ivorycluster is very similar but slightly different from the old
ivorycluster&#8217;s <code>spec.backups.pgbackrest</code> object. The key differences are:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>No image is necessary when restoring from a cloud-based data source</p>
</li>
<li>
<p><code>stanza</code> is a required field when restoring from a cloud-based data source</p>
</li>
<li>
<p><code>backups.pgbackrest</code> has a <code>repos</code> field, which is an array</p>
</li>
<li>
<p><code>dataSource.pgbackrest</code> has a <code>repo</code> field, which is a single object</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note also the similarities:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>We are reusing the secret for both (because the new restore pod needs to have the same credentials as the original backup pod)</p>
</li>
<li>
<p>The <code>repo</code> object is the same</p>
</li>
<li>
<p>The <code>global</code> object is the same</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This is because the new restore pod for the <code>elephant</code> ivorycluster will need to reuse the
configuration and credentials that were originally used in setting up the <code>hippo</code> ivorycluster.</p>
</div>
<div class="paragraph">
<p>In this example, we are creating a new cluster which is also backing up to the same S3 bucket;
only the <code>spec.backups.pgbackrest.global</code> field has changed to point to a different path. This
will ensure that the new <code>elephant</code> cluster will be pre-populated with the data from `hippo&#8217;s
backups, but will backup to its own folders, ensuring that the original backup repository is
appropriately preserved.</p>
</div>
<div class="paragraph">
<p>Deploy this manifest to create the <code>elephant</code> Ivory cluster. Observe that it comes up and running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator describe ivorycluster elephant</code></pre>
</div>
</div>
<div class="paragraph">
<p>When it is ready, you will see that the number of expected instances matches the number of ready
instances, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">Instances:
  Name:               00
  Ready Replicas:     1
  Replicas:           1
  Updated Replicas:   1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The previous example shows how to use an existing S3 repository to pre-populate a ivorycluster
while using a new S3 repository for backing up. But ivoryclusters that use cloud-based data
sources can also use local repositories.</p>
</div>
<div class="paragraph">
<p>For example, assuming a ivorycluster called <code>rhino</code> that was meant to pre-populate from the
original <code>hippo</code> ivorycluster, the manifest would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ivory-operator.ivorysql.org/v1beta1
kind: IvoryCluster
metadata:
  name: rhino
spec:
  image: {{&lt; param imageIvorySQL &gt;}}
  postgresVersion: {{&lt; param postgresVersion &gt;}}
  dataSource:
    pgbackrest:
      stanza: db
      configuration:
      - secret:
          name: ivyo-s3-creds
      global:
        repo1-path: /pgbackrest/ivory-operator/hippo/repo1
      repo:
        name: repo1
        s3:
          bucket: "my-bucket"
          endpoint: "s3.ca-central-1.amazonaws.com"
          region: "ca-central-1"
  instances:
    - dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 1Gi
  backups:
    pgbackrest:
      image: {{&lt; param imagePGBackrest &gt;}}
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 1Gi</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-7"><a class="anchor" href="#next-steps-7"></a>9.9. Next Steps</h3>
<div class="paragraph">
<p>Now we&#8217;ve seen how to clone a cluster and perform a point-in-time-recovery, let&#8217;s see how we can <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/monitoring.md">monitor</a> our Ivory cluster to detect and prevent issues from occurring.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="monitoring"><a class="anchor" href="#monitoring"></a>10. Monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While having <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/high-availability.md">high availability</a> and
<a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/disaster-recovery.md">disaster recovery</a> systems in place helps in the
event of something going wrong with your IvorySQL cluster, monitoring helps you anticipate
problems before they happen. Additionally, monitoring can help you diagnose and resolve issues that
may cause degraded performance rather than downtime.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s look at how IVYO allows you to enable monitoring in your cluster.</p>
</div>
<div class="sect2">
<h3 id="adding-the-exporter-sidecar"><a class="anchor" href="#adding-the-exporter-sidecar"></a>10.1. Adding the Exporter Sidecar</h3>
<div class="paragraph">
<p>Let&#8217;s look at how we can add the IvorySQL Exporter sidecar to your cluster using the
<code>kustomize/ivory</code> example in the <a href="https://github.com/CrunchyData/postgres-operator-examples">Postgres Operator examples</a> repository.</p>
</div>
<div class="paragraph">
<p>Monitoring tools are added using the <code>spec.monitoring</code> section of the custom resource. Currently,
the only monitoring tool supported is the IvorySQL Exporter configured with <a href="https://github.com/CrunchyData/pgmonitor">pgMonitor</a>.</p>
</div>
<div class="paragraph">
<p>In the <code>kustomize/ivory/ivory.yaml</code> file, add the following YAML to the spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">monitoring:
  pgmonitor:
    exporter:
      image: {{&lt; param imagePostgresExporter &gt;}}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Save your changes and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>IVYO will detect the change and add the Exporter sidecar to all Ivory Pods that exist in your
cluster. IVYO will also do the work to allow the Exporter to connect to the database and gather
metrics that can be accessed using the <a href="https://github.com/CrunchyData/postgres-operator-examples/tree/main/kustomize/monitoring">IVYO Monitoring</a> stack.</p>
</div>
<div class="sect3">
<h4 id="configuring-tls-encryption-for-the-exporter"><a class="anchor" href="#configuring-tls-encryption-for-the-exporter"></a>10.1.1. Configuring TLS Encryption for the Exporter</h4>
<div class="paragraph">
<p>IVYO allows you to configure the exporter sidecar to use TLS encryption. If you provide a custom TLS
Secret via the exporter spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  monitoring:
    pgmonitor:
      exporter:
        customTLSSecret:
          name: hippo.tls</code></pre>
</div>
</div>
<div class="paragraph">
<p>Like other custom TLS Secrets that can be configured with IVYO, the Secret will need to be created in
the same Namespace as your PostgresCluster. It should also contain the TLS key (<code>tls.key</code>) and TLS
certificate (<code>tls.crt</code>) needed to enable encryption.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">data:
  tls.crt: &lt;value&gt;
  tls.key: &lt;value&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>After you configure TLS for the exporter, you will need to update your Prometheus deployment to use
TLS, and your connection to the exporter will be encrypted. Check out the <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config">Prometheus</a> documentation
for more information on configuring TLS for <a href="https://prometheus.io/">Prometheus</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="accessing-the-metrics"><a class="anchor" href="#accessing-the-metrics"></a>10.2. Accessing the Metrics</h3>
<div class="paragraph">
<p>Once the IvorySQL Exporter has been enabled in your cluster, follow the steps outlined in
<a href="https://github.com/CrunchyData/postgres-operator-examples/tree/main/kustomize/monitoring">IVYO Monitoring</a> to install the monitoring stack. This will allow you to deploy a <a href="https://github.com/CrunchyData/pgmonitor">pgMonitor</a>
configuration of <a href="https://prometheus.io/">Prometheus</a>, <a href="https://grafana.com/">Grafana</a>, and <a href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alertmanager</a> monitoring tools in Kubernetes. These
tools will be set up by default to connect to the Exporter containers on your Ivory Pods.</p>
</div>
</div>
<div class="sect2">
<h3 id="configurate-monitoring"><a class="anchor" href="#configurate-monitoring"></a>10.3. Configurate Monitoring</h3>
<div class="paragraph">
<p>While the default Kustomize install should work in most Kubernetes environments, it may be
necessary to further customize the project according to your specific needs.</p>
</div>
<div class="paragraph">
<p>For instance, by default <code>fsGroup</code> is set to <code>26</code> for the <code>securityContext</code> defined for the
various Deployments comprising the IVYO Monitoring stack:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">securityContext:
  fsGroup: 26</code></pre>
</div>
</div>
<div class="paragraph">
<p>In most Kubernetes environments this setting is needed to ensure processes within the container
have the permissions needed to write to any volumes mounted to each of the Pods comprising the IVYO
Monitoring stack.  However, when installing in an OpenShift environment (and more specifically when
using the <code>restricted</code> Security Context Constraint), the <code>fsGroup</code> setting should be removed
since OpenShift will automatically handle setting the proper <code>fsGroup</code> within the Pod&#8217;s
<code>securityContext</code>.</p>
</div>
<div class="paragraph">
<p>Additionally, within this same section it may also be necessary to modify the <code>supplmentalGroups</code>
setting according to your specific storage configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">securityContext:
  supplementalGroups : 65534</code></pre>
</div>
</div>
<div class="paragraph">
<p>Therefore, the following files (located under <code>kustomize/monitoring</code>) should be modified and/or
patched (e.g. using additional overlays) as needed to ensure the <code>securityContext</code> is properly
defined for your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>deploy-alertmanager.yaml</code></p>
</li>
<li>
<p><code>deploy-grafana.yaml</code></p>
</li>
<li>
<p><code>deploy-prometheus.yaml</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And to modify the configuration for the various storage resources (i.e. PersistentVolumeClaims)
created by the IVYO Monitoring installer, the <code>kustomize/monitoring/pvcs.yaml</code> file can also
be modified.</p>
</div>
<div class="paragraph">
<p>Additionally, it is also possible to further customize the configuration for the various components
comprising the IVYO Monitoring stack (Grafana, Prometheus and/or AlertManager) by modifying the
following configuration resources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>alertmanager-config.yaml</code></p>
</li>
<li>
<p><code>alertmanager-rules-config.yaml</code></p>
</li>
<li>
<p><code>grafana-datasources.yaml</code></p>
</li>
<li>
<p><code>prometheus-config.yaml</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Finally, please note that the default username and password for Grafana can be updated by
modifying the Grafana Secret in file <code>kustomize/monitoring/grafana-secret.yaml</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="install"><a class="anchor" href="#install"></a>10.4. Install</h3>
<div class="paragraph">
<p>Once the Kustomize project has been modified according to your specific needs, IVYO Monitoring can
then be installed using <code>kubectl</code> and Kustomize:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k kustomize/monitoring</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="uninstall"><a class="anchor" href="#uninstall"></a>10.5. Uninstall</h3>
<div class="paragraph">
<p>And similarly, once IVYO Monitoring has been installed, it can uninstalled using <code>kubectl</code> and
Kustomize:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl delete -k kustomize/monitoring</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-8"><a class="anchor" href="#next-steps-8"></a>10.6. Next Steps</h3>
<div class="paragraph">
<p>Now that we can monitor our cluster, let&#8217;s explore how <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/connection-pooling.md">connection pooling</a> can be enabled using IVYO and how it is helpful.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="connection-pooling"><a class="anchor" href="#connection-pooling"></a>11. Connection Pooling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Connection pooling can be helpful for scaling and maintaining overall availability between your application and the database. IVYO helps facilitate this by supporting the <a href="https://www.pgbouncer.org/">PgBouncer</a> connection pooler and state manager.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s look at how we can a connection pooler and connect it to our application!</p>
</div>
<div class="sect2">
<h3 id="adding-a-connection-pooler"><a class="anchor" href="#adding-a-connection-pooler"></a>11.1. Adding a Connection Pooler</h3>
<div class="paragraph">
<p>Let&#8217;s look at how we can add a connection pooler using the <code>kustomize/keycloak</code> example in the <a href="https://github.com/IvorySQL/ivory-operator">Ivory Operator</a> repository examples folder.</p>
</div>
<div class="paragraph">
<p>Connection poolers are added using the <code>spec.proxy</code> section of the custom resource. Currently, the only connection pooler supported is <a href="https://www.pgbouncer.org/">PgBouncer</a>.</p>
</div>
<div class="paragraph">
<p>The only required attribute for adding a PgBouncer connection pooler is to set the <code>spec.proxy.pgBouncer.image</code> attribute. In the <code>kustomize/keycloak/ivory.yaml</code> file, add the following YAML to the spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">proxy:
  pgBouncer:
    image: {{&lt; param imageIvoryPGBouncer &gt;}}</code></pre>
</div>
</div>
<div class="paragraph">
<p>(You can also find an example of this in the <code>kustomize/examples/high-availability</code> example).</p>
</div>
<div class="paragraph">
<p>Save your changes and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k kustomize/keycloak</code></pre>
</div>
</div>
<div class="paragraph">
<p>IVYO will detect the change and create a new PgBouncer Deployment!</p>
</div>
<div class="paragraph">
<p>That was fairly easy to set up, so now let&#8217;s look at how we can connect our application to the connection pooler.</p>
</div>
</div>
<div class="sect2">
<h3 id="connecting-to-a-connection-pooler"><a class="anchor" href="#connecting-to-a-connection-pooler"></a>11.2. Connecting to a Connection Pooler</h3>
<div class="paragraph">
<p>When a connection pooler is deployed to the cluster, IVYO adds additional information to the user Secrets to allow for applications to connect directly to the connection pooler. Recall that in this example, our user Secret is called <code>keycloakdb-pguser-keycloakdb</code>. Describe the user Secret:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl -n ivory-operator describe secrets keycloakdb-pguser-keycloakdb</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see that there are several new attributes included in this Secret that allow for you to connect to your Ivory instance via the connection pooler:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>pgbouncer-host</code>: The name of the host of the PgBouncer connection pooler.
This references the <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> of the PgBouncer connection pooler.</p>
</li>
<li>
<p><code>pgbouncer-port</code>: The port that the PgBouncer connection pooler is listening on.</p>
</li>
<li>
<p><code>pgbouncer-uri</code>: A <a href="https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING">PostgreSQL connection URI</a>
that provides all the information for logging into the Ivory database via the PgBouncer connection pooler.</p>
</li>
<li>
<p><code>pgbouncer-jdbc-uri</code>: A <a href="https://jdbc.postgresql.org/documentation/use/">PostgreSQL JDBC connection URI</a> that provides
all the information for logging into the Ivory database via the PgBouncer connection pooler using the JDBC driver.
Note that by default, the connection string disable JDBC managing prepared transactions for
<a href="https://www.pgbouncer.org/faq.html#how-to-use-prepared-statements-with-transaction-pooling">optimal use with PgBouncer</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Open up the file in <code>kustomize/keycloak/keycloak.yaml</code>. Update the <code>DB_ADDR</code> and <code>DB_PORT</code> values to be the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: DB_ADDR
  valueFrom: { secretKeyRef: { name: keycloakdb-pguser-keycloakdb, key: pgbouncer-host } }
- name: DB_PORT
  valueFrom: { secretKeyRef: { name: keycloakdb-pguser-keycloakdb, key: pgbouncer-port } }</code></pre>
</div>
</div>
<div class="paragraph">
<p>This changes Keycloak&#8217;s configuration so that it will now connect through the connection pooler.</p>
</div>
<div class="paragraph">
<p>Apply the changes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -k kustomize/keycloak</code></pre>
</div>
</div>
<div class="paragraph">
<p>Kubernetes will detect the changes and begin to deploy a new Keycloak Pod. When it is completed, Keycloak will now be connected to Ivory via the PgBouncer connection pooler!</p>
</div>
</div>
<div class="sect2">
<h3 id="tls"><a class="anchor" href="#tls"></a>11.3. TLS</h3>
<div class="paragraph">
<p>IVYO deploys every cluster and component over TLS. This includes the PgBouncer connection pooler. If you are using your own <a href="#./customize-cluster.md#customize-tls" class="xref unresolved">custom TLS setup</a>, you will need to provide a Secret reference for a TLS key / certificate pair for PgBouncer in <code>spec.proxy.pgBouncer.customTLSSecret</code>.</p>
</div>
<div class="paragraph">
<p>Your TLS certificate for PgBouncer should have a Common Name (CN) setting that matches the PgBouncer Service name. This is the name of the cluster suffixed with <code>-pgbouncer</code>. For example, for our <code>hippo</code> cluster this would be <code>hippo-pgbouncer</code>. For the <code>keycloakdb</code> example, it would be <code>keycloakdb-pgbouncer</code>.</p>
</div>
<div class="paragraph">
<p>To customize the TLS for PgBouncer, you will need to create a Secret in the Namespace of your Ivory cluster that contains the TLS key (<code>tls.key</code>), TLS certificate (<code>tls.crt</code>) and the CA certificate (<code>ca.crt</code>) to use. The Secret should contain the following values:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">data:
  ca.crt: &lt;value&gt;
  tls.crt: &lt;value&gt;
  tls.key: &lt;value&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, if you have files named <code>ca.crt</code>, <code>keycloakdb-pgbouncer.key</code>, and <code>keycloakdb-pgbouncer.crt</code> stored on your local machine, you could run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create secret generic -n ivory-operator keycloakdb-pgbouncer.tls \
  --from-file=ca.crt=ca.crt \
  --from-file=tls.key=keycloakdb-pgbouncer.key \
  --from-file=tls.crt=keycloakdb-pgbouncer.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can specify the custom TLS Secret in the <code>spec.proxy.pgBouncer.customTLSSecret.name</code> field in your <code>ivorycluster.ivory-operator.ivorysql.org</code> custom resource, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  proxy:
    pgBouncer:
      customTLSSecret:
        name: keycloakdb-pgbouncer.tls</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="customizing"><a class="anchor" href="#customizing"></a>11.4. Customizing</h3>
<div class="paragraph">
<p>The PgBouncer connection pooler is highly customizable, both from a configuration and Kubernetes deployment standpoint. Let&#8217;s explore some of the customizations that you can do!</p>
</div>
<div class="sect3">
<h4 id="configuration"><a class="anchor" href="#configuration"></a>11.4.1. Configuration</h4>
<div class="paragraph">
<p><a href="https://www.pgbouncer.org/config.html">PgBouncer configuration</a> can be customized through <code>spec.proxy.pgBouncer.config</code>. After making configuration changes, IVYO will roll them out to any PgBouncer instance and automatically issue a "reload".</p>
</div>
<div class="paragraph">
<p>There are several ways you can customize the configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.proxy.pgBouncer.config.global</code>: Accepts key-value pairs that apply changes globally to PgBouncer.</p>
</li>
<li>
<p><code>spec.proxy.pgBouncer.config.databases</code>: Accepts key-value pairs that represent PgBouncer <a href="https://www.pgbouncer.org/config.html#section-databases">database definitions</a>.</p>
</li>
<li>
<p><code>spec.proxy.pgBouncer.config.users</code>: Accepts key-value pairs that represent <a href="https://www.pgbouncer.org/config.html#section-users">connection settings applied to specific users</a>.</p>
</li>
<li>
<p><code>spec.proxy.pgBouncer.config.files</code>: Accepts a list of files that are mounted in the <code>/etc/pgbouncer</code> directory and loaded before any other options are considered using PgBouncer&#8217;s <a href="https://www.pgbouncer.org/config.html#include-directive">include directive</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, to set the connection pool mode to <code>transaction</code>, you would set the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  proxy:
    pgBouncer:
      config:
        global:
          pool_mode: transaction</code></pre>
</div>
</div>
<div class="paragraph">
<p>For a reference on <a href="https://www.pgbouncer.org/config.html">PgBouncer configuration</a> please see:</p>
</div>
<div class="paragraph">
<p><a href="https://www.pgbouncer.org/config.html" class="bare">https://www.pgbouncer.org/config.html</a></p>
</div>
</div>
<div class="sect3">
<h4 id="replicas"><a class="anchor" href="#replicas"></a>11.4.2. Replicas</h4>
<div class="paragraph">
<p>IVYO deploys one PgBouncer instance by default. You may want to run multiple PgBouncer instances to have some level of redundancy, though you still want to be mindful of how many connections are going to your Ivory database!</p>
</div>
<div class="paragraph">
<p>You can manage the number of PgBouncer instances that are deployed through the <code>spec.proxy.pgBouncer.replicas</code> attribute.</p>
</div>
</div>
<div class="sect3">
<h4 id="resources"><a class="anchor" href="#resources"></a>11.4.3. Resources</h4>
<div class="paragraph">
<p>You can manage the CPU and memory resources given to a PgBouncer instance through the <code>spec.proxy.pgBouncer.resources</code> attribute. The layout of <code>spec.proxy.pgBouncer.resources</code> should be familiar: it follows the same pattern as the standard Kubernetes structure for setting <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">container resources</a>.</p>
</div>
<div class="paragraph">
<p>For example, let&#8217;s say we want to set some CPU and memory limits on our PgBouncer instances. We could add the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  proxy:
    pgBouncer:
      resources:
        limits:
          cpu: 200m
          memory: 128Mi</code></pre>
</div>
</div>
<div class="paragraph">
<p>As IVYO deploys the PgBouncer instances using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> these changes are rolled out using a rolling update to minimize disruption between your application and Ivory instances!</p>
</div>
</div>
<div class="sect3">
<h4 id="annotations-labels"><a class="anchor" href="#annotations-labels"></a>11.4.4. Annotations / Labels</h4>
<div class="paragraph">
<p>You can apply custom annotations and labels to your PgBouncer instances through the <code>spec.proxy.pgBouncer.metadata.annotations</code> and <code>spec.proxy.pgBouncer.metadata.labels</code> attributes respectively. Note that any changes to either of these two attributes take precedence over any other custom labels you have added.</p>
</div>
</div>
<div class="sect3">
<h4 id="pod-anti-affinity-pod-affinity-node-affinity"><a class="anchor" href="#pod-anti-affinity-pod-affinity-node-affinity"></a>11.4.5. Pod Anti-Affinity / Pod Affinity / Node Affinity</h4>
<div class="paragraph">
<p>You can control the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod anti-affinity, pod affinity, and node affinity</a> through the <code>spec.proxy.pgBouncer.affinity</code> attribute, specifically:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.proxy.pgBouncer.affinity.nodeAffinity</code>: controls node affinity for the PgBouncer instances.</p>
</li>
<li>
<p><code>spec.proxy.pgBouncer.affinity.podAffinity</code>: controls Pod affinity for the PgBouncer instances.</p>
</li>
<li>
<p><code>spec.proxy.pgBouncer.affinity.podAntiAffinity</code>: controls Pod anti-affinity for the PgBouncer instances.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each of the above follows the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">standard Kubernetes specification for setting affinity</a>.</p>
</div>
<div class="paragraph">
<p>For example, to set a preferred Pod anti-affinity rule for the <code>kustomize/keycloak</code> example, you would want to add the following to your configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  proxy:
    pgBouncer:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  ivory-operator.ivorysql.org/cluster: keycloakdb
                  ivory-operator.ivorysql.org/role: pgbouncer
              topologyKey: kubernetes.io/hostname</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="tolerations"><a class="anchor" href="#tolerations"></a>11.4.6. Tolerations</h4>
<div class="paragraph">
<p>You can deploy PgBouncer instances to <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Nodes with Taints</a> by setting <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Tolerations</a> through <code>spec.proxy.pgBouncer.tolerations</code>. This attribute follows the Kubernetes standard tolerations layout.</p>
</div>
<div class="paragraph">
<p>For example, if there were a set of Nodes with a Taint of <code>role=connection-poolers:NoSchedule</code> that you want to schedule your PgBouncer instances to, you could apply the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  proxy:
    pgBouncer:
      tolerations:
      - effect: NoSchedule
        key: role
        operator: Equal
        value: connection-poolers</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that setting a toleration does not necessarily mean that the PgBouncer instances will be assigned to Nodes with those taints. Tolerations act as a <strong>key</strong>: they allow for you to access Nodes. If you want to ensure that your PgBouncer instances are deployed to specific nodes, you need to combine setting tolerations with node affinity.</p>
</div>
</div>
<div class="sect3">
<h4 id="pod-spread-constraints"><a class="anchor" href="#pod-spread-constraints"></a>11.4.7. Pod Spread Constraints</h4>
<div class="paragraph">
<p>Besides using affinity, anti-affinity and tolerations, you can also set <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Topology Spread Constraints</a> through <code>spec.proxy.pgBouncer.topologySpreadConstraints</code>. This attribute follows the Kubernetes standard topology spread contraint layout.</p>
</div>
<div class="paragraph">
<p>For example, since each of of our pgBouncer Pods will have the standard <code>ivory-operator.ivorysql.org/role: pgbouncer</code> Label set, we can use this Label when determining the <code>maxSkew</code>. In the example below, since we have 3 nodes with a <code>maxSkew</code> of 1 and we&#8217;ve set <code>whenUnsatisfiable</code> to <code>ScheduleAnyway</code>, we should ideally see 1 Pod on each of the nodes, but our Pods can be distributed less evenly if other constraints keep this from happening.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  proxy:
    pgBouncer:
      replicas: 3
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: my-node-label
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              ivory-operator.ivorysql.org/role: pgbouncer</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you want to ensure that your PgBouncer instances are deployed more evenly (or not deployed at all), you need to update <code>whenUnsatisfiable</code> to <code>DoNotSchedule</code>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-9"><a class="anchor" href="#next-steps-9"></a>11.5. Next Steps</h3>
<div class="paragraph">
<p>Now that we can enable connection pooling in a cluster, Let&#8217;s explore some <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/administrative-tasks.md">administrative tasks</a> such as manually restarting IvorySQL using IVYO. How do we do that?</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="administrative-tasks"><a class="anchor" href="#administrative-tasks"></a>12. Administrative Tasks</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="manually-restarting-ivorysql"><a class="anchor" href="#manually-restarting-ivorysql"></a>12.1. Manually Restarting IvorySQL</h3>
<div class="paragraph">
<p>There are times when you might need to manually restart IvorySQL. This can be done by adding or updating a custom annotation to the cluster&#8217;s <code>spec.metadata.annotations</code> section. IVYO will notice the change and perform a <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/architecture/high-availability.md#rolling-update">rolling restart</a>.</p>
</div>
<div class="paragraph">
<p>For example, if you have a cluster named <code>hippo</code> in the namespace <code>ivory-operator</code>, all you need to do is patch the hippo ivorycluster with the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl patch ivorycluster/hippo -n ivory-operator --type merge \
  --patch '{"spec":{"metadata":{"annotations":{"restarted":"'"$(date)"'"}}}}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Watch your hippo cluster: you will see the rolling update has been triggered and the restart has begun.</p>
</div>
</div>
<div class="sect2">
<h3 id="shutdown"><a class="anchor" href="#shutdown"></a>12.2. Shutdown</h3>
<div class="paragraph">
<p>You can shut down an Ivory cluster by setting the <code>spec.shutdown</code> attribute to <code>true</code>. You can do this by editing the manifest, or, in the case of the <code>hippo</code> cluster, executing a command like the below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl patch ivorycluster/hippo -n ivory-operator --type merge \
  --patch '{"spec":{"shutdown": true}}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>The effect of this is that all the Kubernetes workloads for this cluster are
scaled to 0. You can verify this with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get deploy,sts,cronjob --selector=ivory-operator.ivorysql.org/cluster=hippo -n ivory-operator

NAME                             READY   AGE
statefulset.apps/hippo-00-lwgx   0/0     1h

NAME                             SCHEDULE   SUSPEND   ACTIVE
cronjob.batch/hippo-repo1-full   @daily     True      0</code></pre>
</div>
</div>
<div class="paragraph">
<p>To turn an Ivory cluster that is shut down back on, you can set <code>spec.shutdown</code> to <code>false</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="pausing-reconciliation-and-rollout"><a class="anchor" href="#pausing-reconciliation-and-rollout"></a>12.3. Pausing Reconciliation and Rollout</h3>
<div class="paragraph">
<p>You can pause the Ivory cluster reconciliation process by setting the
<code>spec.paused</code> attribute to <code>true</code>. You can do this by editing the manifest, or,
in the case of the <code>hippo</code> cluster, executing a command like the below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl patch ivorycluster/hippo -n ivory-operator --type merge \
  --patch '{"spec":{"paused": true}}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Pausing a cluster will suspend any changes to the cluster&#8217;s current state until
reconciliation is resumed. This allows you to fully control when changes to
the ivorycluster spec are rolled out to the Ivory cluster. While paused,
no statuses are updated other than the "Progressing" condition.</p>
</div>
<div class="paragraph">
<p>To resume reconciliation of an Ivory cluster, you can either set <code>spec.paused</code>
to <code>false</code> or remove the setting from your manifest.</p>
</div>
</div>
<div class="sect2">
<h3 id="rotating-tls-certificates"><a class="anchor" href="#rotating-tls-certificates"></a>12.4. Rotating TLS Certificates</h3>
<div class="paragraph">
<p>Credentials should be invalidated and replaced (rotated) as often as possible
to minimize the risk of their misuse. Unlike passwords, every TLS certificate
has an expiration, so replacing them is inevitable.</p>
</div>
<div class="paragraph">
<p>In fact, IVYO automatically rotates the client certificates that it manages <strong>before</strong>
the expiration date on the certificate. A new client certificate will be generated
after 2/3rds of its working duration; so, for instance, a IVYO-created certificate
with an expiration date 12 months in the future will be replaced by IVYO around the
eight month mark. This is done so that you do not have to worry about running into
problems or interruptions of service with an expired certificate.</p>
</div>
<div class="sect3">
<h4 id="triggering-a-certificate-rotation"><a class="anchor" href="#triggering-a-certificate-rotation"></a>12.4.1. Triggering a Certificate Rotation</h4>
<div class="paragraph">
<p>If you want to rotate a single client certificate, you can regenerate the certificate
of an existing cluster by deleting the <code>tls.key</code> field from its certificate Secret.</p>
</div>
<div class="paragraph">
<p>Is it time to rotate your IVYO root certificate? All you need to do is delete the <code>ivyo-root-cacert</code> secret. IVYO will regenerate it and roll it out seamlessly, ensuring your apps continue communicating with the Ivory cluster without having to update any configuration or deal with any downtime.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl delete secret ivyo-root-cacert</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>IVYO only updates secrets containing the generated root certificate. It does not touch custom certificates.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="rotating-custom-tls-certificates"><a class="anchor" href="#rotating-custom-tls-certificates"></a>12.4.2. Rotating Custom TLS Certificates</h4>
<div class="paragraph">
<p>When you use your own TLS certificates with IVYO, you are responsible for replacing them appropriately.
Here&#8217;s how.</p>
</div>
<div class="paragraph">
<p>IVYO automatically detects and loads changes to the contents of IvorySQL server
and replication Secrets without downtime. You or your certificate manager need
only replace the values in the Secret referenced by <code>spec.customTLSSecret</code>.</p>
</div>
<div class="paragraph">
<p>If instead you change <code>spec.customTLSSecret</code> to refer to a new Secret or new fields,
IVYO will perform a <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/architecture/high-availability.md#rolling-update">rolling restart</a>.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When changing the IvorySQL certificate authority, make sure to update
<a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/customize-cluster.md#customize-tls"><code>customReplicationTLSSecret</code></a> as well.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="changing-the-primary"><a class="anchor" href="#changing-the-primary"></a>12.5. Changing the Primary</h3>
<div class="paragraph">
<p>There may be times when you want to change the primary in your HA cluster. This can be done
using the <code>patroni.switchover</code> section of the ivorycluster spec. It allows
you to enable switchovers in your ivoryclusters, target a specific instance as the new
primary, and run a failover if your ivorycluster has entered a bad state.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s go through the process of performing a switchover!</p>
</div>
<div class="paragraph">
<p>First you need to update your spec to prepare your cluster to change the primary. Edit your spec
to have the following fields:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    switchover:
      enabled: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>After you apply this change, IVYO will be looking for the trigger to perform a switchover in your
cluster. You will trigger the switchover by adding the <code>ivory-operator.ivorysql.org/trigger-switchover</code>
annotation to your custom resource. The best way to set this annotation is
with a timestamp, so you know when you initiated the change.</p>
</div>
<div class="paragraph">
<p>For example, for our <code>hippo</code> cluster, we can run the following command to trigger the switchover:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl annotate -n ivory-operator ivorycluster hippo \
  ivory-operator.ivorysql.org/trigger-switchover="$(date)"</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you want to perform another switchover you can re-run the annotation command and add the <code>--overwrite</code> flag:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl annotate -n ivory-operator ivorycluster hippo --overwrite \
  ivory-operator.ivorysql.org/trigger-switchover="$(date)"</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>IVYO will detect this annotation and use the Patroni API to request a change to the current primary!</p>
</div>
<div class="paragraph">
<p>The roles on your database instance Pods will start changing as Patroni works. The new primary
will have the <code>master</code> role label, and the old primary will be updated to <code>replica</code>.</p>
</div>
<div class="paragraph">
<p>The status of the switch will be tracked using the <code>status.patroni.switchover</code> field. This will be set
to the value defined in your trigger annotation. If you use a timestamp as the annotation this is
another way to determine when the switchover was requested.</p>
</div>
<div class="paragraph">
<p>After the instance Pod labels have been updated and <code>status.patroni.switchover</code> has been set, the
primary has been changed on your cluster!</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>After changing the primary, we recommend that you disable switchovers by setting <code>spec.patroni.switchover.enabled</code>
to false or remove the field from your spec entirely. If the field is removed the corresponding
status will also be removed from the ivorycluster.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="targeting-an-instance"><a class="anchor" href="#targeting-an-instance"></a>12.5.1. Targeting an instance</h4>
<div class="paragraph">
<p>Another option you have when switching the primary is providing a target instance as the new
primary. This target instance will be used as the candidate when performing the switchover.
The <code>spec.patroni.switchover.targetInstance</code> field takes the name of the instance that you are switching to.</p>
</div>
<div class="paragraph">
<p>This name can be found in a couple different places; one is as the name of the StatefulSet and
another is on the database Pod as the <code>ivory-operator.ivorysql.org/instance</code> label. The
following commands can help you determine who is the current primary and what name to use as the
<code>targetInstance</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell-session hljs" data-lang="shell-session">$ kubectl get pods -l ivory-operator.ivorysql.org/cluster=hippo \
    -L ivory-operator.ivorysql.org/instance \
    -L ivory-operator.ivorysql.org/role -n ivory-operator

NAME                      READY   STATUS      RESTARTS   AGE     INSTANCE               ROLE
hippo-instance1-jdb5-0    3/3     Running     0          2m47s   hippo-instance1-jdb5   master
hippo-instance1-wm5p-0    3/3     Running     0          2m47s   hippo-instance1-wm5p   replica</code></pre>
</div>
</div>
<div class="paragraph">
<p>In our example cluster <code>hippo-instance1-jdb5</code> is currently the primary meaning we want to target
<code>hippo-instance1-wm5p</code> in the switchover. Now that you know which instance is currently the
primary and how to find your <code>targetInstance</code>, let&#8217;s update your cluster spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    switchover:
      enabled: true
      targetInstance: hippo-instance1-wm5p</code></pre>
</div>
</div>
<div class="paragraph">
<p>After applying this change you will once again need to trigger the switchover by annotating the
ivorycluster (see above commands). You can verify the switchover has completed by checking the
Pod role labels and <code>status.patroni.switchover</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="failover"><a class="anchor" href="#failover"></a>12.5.2. Failover</h4>
<div class="paragraph">
<p>Finally, we have the option to failover when your cluster has entered an unhealthy state. The
only spec change necessary to accomplish this is updating the <code>spec.patroni.switchover.type</code>
field to the <code>Failover</code> type. One note with this is that a <code>targetInstance</code> is required when
performing a failover. Based on the example cluster above, assuming <code>hippo-instance1-wm5p</code> is still
a replica, we can update the spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  patroni:
    switchover:
      enabled: true
      targetInstance: hippo-instance1-wm5p
      type: Failover</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply this spec change and your ivorycluster will be prepared to perform the failover. Again
you will need to trigger the switchover by annotating the ivorycluster (see above commands)
and verify that the Pod role labels and <code>status.patroni.switchover</code> are updated accordingly.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Errors encountered in the switchover process can leave your cluster in a bad
state. If you encounter issues, found in the operator logs, you can update the spec to fix the
issues and apply the change. Once the change has been applied, IVYO will attempt to perform the
switchover again.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="next-steps-10"><a class="anchor" href="#next-steps-10"></a>12.6. Next Steps</h3>
<div class="paragraph">
<p>We&#8217;ve covered a lot in terms of building, maintaining, scaling, customizing, restarting, and expanding our Ivory cluster. However, there may come a time where we need to <a href="https://github.com/IvorySQL/ivory-operator/blob/master/docs/content/tutorial/delete-cluster.md">delete our Ivory cluster</a>. How do we do that?</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="delete-an-ivory-cluster"><a class="anchor" href="#delete-an-ivory-cluster"></a>13. Delete an Ivory Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There comes a time when it is necessary to delete your cluster. If you have been following along with the example, you can delete your Ivory cluster by simply running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl delete -k examples/kustomize/ivory</code></pre>
</div>
</div>
<div class="paragraph">
<p>IVYO will remove all of the objects associated with your cluster.</p>
</div>
<div class="paragraph">
<p>With data retention, this is subject to the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming">retention policy of your PVC</a>. For more information on how Kubernetes manages data retention, please refer to the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming">Kubernetes docs on volume reclaiming</a>.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="4.6.1.html">K8S deployment</a></span>
  <span class="next"><a href="4.6.4.html">Docker &amp; Podman deployment</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">

</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
<script src="../../_/js/vendor/lunr.js"></script>
<script src="../../_/js/search-ui.js" id="search-ui-script" data-site-root-path="../.." data-snippet-length="100" data-stylesheet="../../_/css/search.css"></script>
<script async src="../../search-index.js"></script>
  </body>
</html>
